{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a4f59f4a-f822-4f61-9f0d-448f1cda3eaf","_uuid":"a08a27cb-7612-4fae-9525-76ed59ed3a34","id":"y55SgbHXJHg-","trusted":true},"source":["## Importacion liberias"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f12106e-c3ea-45ab-9bb7-58a57db899f5","_uuid":"a83cd613-555b-4c64-aacf-940e31bfe11e","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:28.470086Z","iopub.status.busy":"2024-05-29T07:53:28.469219Z","iopub.status.idle":"2024-05-29T07:53:40.923895Z","shell.execute_reply":"2024-05-29T07:53:40.922974Z","shell.execute_reply.started":"2024-05-29T07:53:28.470051Z"},"executionInfo":{"elapsed":5837,"status":"ok","timestamp":1716911157404,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"vTRhvLhdl6fI","jupyter":{"outputs_hidden":false},"outputId":"e7b552d6-bf93-4c76-a501-b697f1534730","trusted":true},"outputs":[],"source":["!pip install torch numpy pandas matplotlib scikit-learn transformers datasets torcheval accelerate"]},{"cell_type":"markdown","metadata":{},"source":["**Importación librerias**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"925d9ae6-a7e8-49a9-b15d-221f11e2a8c6","_uuid":"ec6391b1-de70-444e-83a8-4460655bbe9f","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:59:14.906611Z","iopub.status.busy":"2024-05-29T07:59:14.905953Z","iopub.status.idle":"2024-05-29T07:59:14.913400Z","shell.execute_reply":"2024-05-29T07:59:14.912528Z","shell.execute_reply.started":"2024-05-29T07:59:14.906574Z"},"executionInfo":{"elapsed":7597,"status":"ok","timestamp":1716911164998,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"UvAT1bj3HCOd","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","#from torch.utils.data import Dataset, DataLoader\n","import os\n","import re\n","import sys\n","import json\n","from dotenv import load_dotenv\n","import numpy as np\n","import pandas as pd\n","import datasets as ds\n","import matplotlib.pyplot as plt\n","from datetime import datetime, timezone\n","from transformers import (\n","    AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoConfig, EarlyStoppingCallback\n",")\n","from transformers.integrations import TensorBoardCallback\n","from transformers import EvalPrediction\n","from sklearn.metrics import f1_score, recall_score, precision_score\n","from torcheval.metrics import MulticlassAccuracy\n","from typing import Dict, List, Tuple, Union, Optional, Callable, ClassVar\n","from typing import ClassVar\n","from datasets import DatasetDict, load_dataset\n","from sklearn.model_selection import train_test_split\n","import ppscore as pps\n","from transformers import DataCollatorWithPadding\n","from IPython.display import display, clear_output, update_display\n","from tqdm.notebook import tqdm\n","from IPython.display import display, clear_output\n","from dataclasses import dataclass, field\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import seaborn as sns\n","# NLP\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","nltk.download('wordnet')\n","\n","# Para habilitar la depuración de errores CUDA\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","# API KEYS IMPORT\n","load_dotenv()\n","os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"6a6d0294-bbfa-4cbd-8319-87df5aed3302","_uuid":"85d35f16-ee2f-42af-b660-867a8ed0f684","id":"Mvz1Mb-HlzF9","trusted":true},"source":["Versiones, información del sistema y del device"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d7d652c-ffdc-44d2-8a38-55c09e7072dd","_uuid":"c0f11c88-01ba-45cf-b656-a60f70e3b1ba","collapsed":false,"executionInfo":{"elapsed":419,"status":"ok","timestamp":1716911177376,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"WCMDJ2hrlzF9","jupyter":{"outputs_hidden":false},"outputId":"4e8bc981-d997-45ac-f3ce-89fb00392dd4","trusted":true},"outputs":[],"source":["print(sys.executable)\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","print(f\"CUDA version: {torch.version.cuda}\")\n","print(f\"Device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n","print(torch.cuda.get_arch_list())\n","\n","# sys, library info\n","print(\"Operating System:\", os.name)\n","print(\"Platform:\", sys.platform)\n","print(\"Current Working Directory:\", os.getcwd())\n","print(\"Environment Variables:\", os.environ)\n","print(\"Python Version:\", sys.version)\n","print(\"Command-line arguments:\", sys.argv)\n","print(ds.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d698cc1-79df-400a-9c1b-03a3aea375b2","_uuid":"feef1da2-47a5-46fe-9b64-b134011d4a8e","collapsed":false,"executionInfo":{"elapsed":1732,"status":"ok","timestamp":1716911181095,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"0CAMp28-mUQ1","jupyter":{"outputs_hidden":false},"outputId":"0fc71155-0bd1-468b-8af3-4e8fe5f8dd28","trusted":true},"outputs":[],"source":["\"\"\"\n","# Descomentar para importaciones en google colab \n","\n","# Importacion del drive content\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Importacion de clase propia \"Eda\" en colab\n","ruta_modulo_vis_colab = \"/content/drive/MyDrive/NLP/code/\"\n","sys.path.append(os.path.abspath(ruta_modulo_vis_colab))\n","from visualization import Eda\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# GPU INFO\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ab322ae-47cc-43a1-8819-3edbf01384bc","_uuid":"fc31a5df-88ae-4354-918d-dd152c4f4ba7","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:48.267737Z","iopub.status.busy":"2024-05-29T07:53:48.267491Z","iopub.status.idle":"2024-05-29T07:53:48.320034Z","shell.execute_reply":"2024-05-29T07:53:48.319099Z","shell.execute_reply.started":"2024-05-29T07:53:48.267716Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716911183160,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"0LTMeUYoGYY8","jupyter":{"outputs_hidden":false},"outputId":"7acc305e-0d2c-4c0e-f8a2-738f0f6fe0fb","trusted":true},"outputs":[],"source":["# VER SI GPU ESTA DISPONIBLE PARA ENTRENAR\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"markdown","metadata":{"_cell_guid":"82dcbe46-2f0f-46f2-9435-8cfb83a7dc7b","_uuid":"175d5bd5-fe7b-4d5d-aaf5-167215424fbc","id":"Jl-9BYXRGyEW","trusted":true},"source":["## **EDA**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ebdad3f-4289-4561-ab30-cba108d0bfcf","_uuid":"e0d1a855-3b6d-4692-8736-70520a2d67ce","collapsed":false,"executionInfo":{"elapsed":246,"status":"ok","timestamp":1716911187339,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"FDg0ufSqnkj6","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Importacion de clase propia \"Eda\" en local\n","sys.path.append(os.path.abspath('./code'))\n","from visualization import Eda"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14d7a405-1909-4052-a5d9-8380adedbda3","_uuid":"e923c71c-ea24-474c-9eab-85659744c29e","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:48.332337Z","iopub.status.busy":"2024-05-29T07:53:48.332055Z","iopub.status.idle":"2024-05-29T07:53:48.424138Z","shell.execute_reply":"2024-05-29T07:53:48.423388Z","shell.execute_reply.started":"2024-05-29T07:53:48.332314Z"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1716911189999,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"LBqNpU88G0eP","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Importacion de los datos\n","ruta_data_colab  = \"/content/drive/MyDrive/NLP/data/data_tw.csv\"\n","ruta_data_local = \"./data/data_tw.csv\"\n","ruta_data_kaggle = \"/kaggle/input/data-tw/data_tw.csv\"\n","\n","df = pd.read_csv(ruta_data_local)\n","df.drop(columns = \t'Unnamed: 0', inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b63f0a3b-e0bd-4236-b042-978ccb6b7575","_uuid":"e34c991c-036b-4ed4-90ab-b1f08b39e177","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:48.425476Z","iopub.status.busy":"2024-05-29T07:53:48.425173Z","iopub.status.idle":"2024-05-29T07:53:48.439554Z","shell.execute_reply":"2024-05-29T07:53:48.438629Z","shell.execute_reply.started":"2024-05-29T07:53:48.425454Z"},"executionInfo":{"elapsed":2736,"status":"ok","timestamp":1716911195165,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"wV7TM6F_JjnY","jupyter":{"outputs_hidden":false},"outputId":"6f059430-63a6-4888-ac2a-5f1d1a8a2d2b","trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Número de muestras de cada clase a predecir**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"712a9aa2-1f08-4ca8-9335-2c5ef217abd9","_uuid":"e777c730-c267-4190-b4ed-ed13980703c8","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:48.440788Z","iopub.status.busy":"2024-05-29T07:53:48.440537Z","iopub.status.idle":"2024-05-29T07:53:48.449583Z","shell.execute_reply":"2024-05-29T07:53:48.448630Z","shell.execute_reply.started":"2024-05-29T07:53:48.440766Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716911195165,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"6VT97Q_CKhMY","jupyter":{"outputs_hidden":false},"outputId":"505651ff-8c8b-4f9a-824f-68837f59bebc","trusted":true},"outputs":[],"source":["df[\"label\"].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["**Columnas del dataset: features y target**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e723cd47-81ce-4c4b-b35b-da9557308144","_uuid":"4a96f1ba-1f15-470c-aec3-153d2d8e84f0","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:48.453567Z","iopub.status.busy":"2024-05-29T07:53:48.453199Z","iopub.status.idle":"2024-05-29T07:53:48.460278Z","shell.execute_reply":"2024-05-29T07:53:48.459312Z","shell.execute_reply.started":"2024-05-29T07:53:48.453537Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716907661347,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"YXid97i5Jl0N","jupyter":{"outputs_hidden":false},"outputId":"051fbb9d-6706-46a5-8c5c-1a2d4a3e2ce6","trusted":true},"outputs":[],"source":["df.columns"]},{"cell_type":"markdown","metadata":{},"source":["**Número de muestras y número de variables**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc5f9289-20f2-4639-993c-09cc4ce6c927","_uuid":"8a831322-a427-45eb-b121-e2b856265a42","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:48.461776Z","iopub.status.busy":"2024-05-29T07:53:48.461488Z","iopub.status.idle":"2024-05-29T07:53:48.467714Z","shell.execute_reply":"2024-05-29T07:53:48.466718Z","shell.execute_reply.started":"2024-05-29T07:53:48.461754Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716907661654,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"2U312sXUKpYG","jupyter":{"outputs_hidden":false},"outputId":"a310aa28-eecf-47aa-8d29-f6da01b82a02","trusted":true},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","metadata":{},"source":["**Añaden etiquetas para clase 0 y 1 de tipo string, las llamaremos 0 -> NEGATIVE Y 1 -> POSITIVE**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = {\n","        0: \"NEGATIVE\" ,\n","         1:\"POSITIVE\"\n","}\n","df[\"label_name\"] = df[\"label\"].map(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"post_text\"][df[\"label_name\"] == \"NEGATIVE\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"post_text\"][df[\"label_name\"] == \"POSITIVE\"]\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"5f4da82c-6181-473d-a03f-753a8d217cb7","_uuid":"583576d1-664b-4ee2-a7ce-6d9ac1213aa8","id":"bm6ACwWZlzF_","trusted":true},"source":["**Antes de nada se van a crear nuevas columnas de año,mes,dia,hora ... a apartir de fecha publicacion del post para analizar mejor los datos y crear ciertas gráficos clusterizados por meses, años, dia de la semana ...**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eaf04564-5532-48cc-bb8f-defe9b7b5124","_uuid":"c8c76f12-8bb0-4ac6-b1ae-ee193a31df5e","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:49.481844Z","iopub.status.busy":"2024-05-29T07:53:49.481485Z","iopub.status.idle":"2024-05-29T07:53:54.968322Z","shell.execute_reply":"2024-05-29T07:53:54.967199Z","shell.execute_reply.started":"2024-05-29T07:53:49.481817Z"},"executionInfo":{"elapsed":5755,"status":"ok","timestamp":1716911206362,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"h2ugiQfVlzF_","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def extract_date_components(date_string):\n","    format_string = \"%a %b %d %H:%M:%S %z %Y\"\n","    dt = datetime.strptime(date_string, format_string)\n","    return pd.Series({\n","        'dia': str(dt.day),\n","        'mes': str(dt.month),\n","        'año': str(dt.year),\n","        'h': dt.hour,\n","        'min': dt.minute,\n","        'seg': dt.second,\n","        'dia_semana': dt.strftime(\"%A\"),\n","        'nombre_mes': dt.strftime(\"%B\")\n","    })\n","date_components_df = df['post_created'].apply(extract_date_components)\n","df = pd.concat([df, date_components_df], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1fc504f-332a-47c8-9709-1b5242db39c7","_uuid":"b4d50982-c25b-446f-92d0-6297c524db5c","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:54.970395Z","iopub.status.busy":"2024-05-29T07:53:54.970090Z","iopub.status.idle":"2024-05-29T07:53:54.976313Z","shell.execute_reply":"2024-05-29T07:53:54.975540Z","shell.execute_reply.started":"2024-05-29T07:53:54.970371Z"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1716911206362,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"0WheWl9clzF_","jupyter":{"outputs_hidden":false},"outputId":"2f0b57e0-3ff0-4e81-d65f-daa41d0308c1","trusted":true},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d272624b-8970-4900-b45a-7e7f060b98db","_uuid":"0badb73e-71c9-4c90-8afa-aa9db687565c","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:54.977901Z","iopub.status.busy":"2024-05-29T07:53:54.977531Z","iopub.status.idle":"2024-05-29T07:53:54.997925Z","shell.execute_reply":"2024-05-29T07:53:54.997019Z","shell.execute_reply.started":"2024-05-29T07:53:54.977865Z"},"executionInfo":{"elapsed":2224,"status":"ok","timestamp":1716911208575,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"eQEFYdrulzF_","jupyter":{"outputs_hidden":false},"outputId":"05c1533d-fbe3-4ddc-83ba-f9404479b825","trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Utilización de la clase Eda propia y sus métodos para la visualización y creación de varios tipos de diagramas**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"999a5a88-d134-4b63-a3b8-9af502b439db","_uuid":"37ee1a20-f9c6-4ace-82d7-db43009e8948","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:53:55.000034Z","iopub.status.busy":"2024-05-29T07:53:54.999728Z","iopub.status.idle":"2024-05-29T07:53:55.698610Z","shell.execute_reply":"2024-05-29T07:53:55.697319Z","shell.execute_reply.started":"2024-05-29T07:53:55.000010Z"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1716911209904,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"pZqJ-VLVlzF_","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["tw_eda = Eda(data = df, auto_eda = False, target_var  = \"label\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f92326ba-83e4-4fdd-9b97-edb7c4229fb5","_uuid":"3b22c13b-2c77-4dd7-b674-f2eabe9c1ad9","collapsed":false,"executionInfo":{"elapsed":1,"status":"ok","timestamp":1716907694735,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"Z_rWbNmulzGA","jupyter":{"outputs_hidden":false},"outputId":"b6fd5ac6-cc39-47ce-f6ec-10232a227077","trusted":true},"outputs":[],"source":["# Columnas categoricas del df\n","tw_eda.cat_cols"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Columnas numericas del df\n","tw_eda.num_cols"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Informacion de las columnas categoricas\n","tw_eda.infocat"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f69e27a-fe58-43a1-87a1-19d3856cc714","_uuid":"4c55aa5d-821b-407d-8649-43ba4a24bb40","collapsed":false,"executionInfo":{"elapsed":2463,"status":"ok","timestamp":1716907698757,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"ThpiKLHWlzGA","jupyter":{"outputs_hidden":false},"outputId":"b96d9e27-c110-41d3-835f-8333c7cb4d38","trusted":true},"outputs":[],"source":["# Informacion de las columnas numericas\n","tw_eda.infonum"]},{"cell_type":"markdown","metadata":{"_cell_guid":"1b09e77c-eebd-4e78-8e44-56aca6d1231f","_uuid":"fa5db714-cc3d-4680-bad9-6bb91f474702","id":"CufkBcu5lzGA","trusted":true},"source":["**Antes de empezar con las gráficas y debido a las diferentes escalas de las variables númericas, se estandarizán estas y se dropean el user_id, label , el post_id ... para que no se vean afectadas por la estandarización [resta la media y divide entre la varianza de cada feature por separado]**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3149aa01-1c91-402a-b274-4be6711bf79b","_uuid":"38c31192-600d-40aa-b934-8c08b29157e7","collapsed":false,"executionInfo":{"elapsed":313,"status":"ok","timestamp":1716911215669,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"qQV89eGslzGA","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["num_data_clean = tw_eda.num_data.drop(columns = [\"user_id\",\"post_id\",'label','h','min','seg'])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a4e3dab-2463-4fd5-ae39-f7f6a97aa48e","_uuid":"68bcf037-6f80-4bc4-ba0e-9fe0d1abf2b2","collapsed":false,"executionInfo":{"elapsed":1,"status":"ok","timestamp":1716911216328,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"kblrDLtLlzGA","jupyter":{"outputs_hidden":false},"outputId":"acd267ec-2efd-4a0b-b573-f4ef8e67b20b","trusted":true},"outputs":[],"source":["num_data_clean.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6bd4116-fa20-4746-b7b6-2002caa69122","_uuid":"bac4c7de-b5f8-4f01-ad21-be8e11abfa45","collapsed":false,"executionInfo":{"elapsed":1,"status":"ok","timestamp":1716911216951,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"LrCkszvqlzGA","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Estandarización mediante sklearn\n","from sklearn.preprocessing import StandardScaler\n","std = StandardScaler()\n","std_data = std.fit_transform(num_data_clean)\n","std_dataframe = pd.DataFrame(data = std_data ,columns = num_data_clean.columns.tolist())\n","tw_eda_std = Eda(data = std_dataframe)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45edef4c-bd07-4e85-b055-6e87e6642174","_uuid":"906ffcdf-5f50-4221-ba96-3edb0c816836","collapsed":false,"id":"9umQSSnHlzGA","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Exploramos el cambio de escala aplicado\n","tw_eda_std.infonum"]},{"cell_type":"markdown","metadata":{},"source":["**Histogramas**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bce46bf5-bbbb-452d-ae1e-0209607b04aa","_uuid":"17bc90d2-394b-46b4-bd35-000b12ca5126","collapsed":false,"id":"K9nYm6pxlzGA","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Histogramas de las varaiables numericas\n","tw_eda.plot_hist(\n","                        bins = 30 ,\n","                        divide_feature = None,\n","                        layout = 'constrained',\n","                        fig_x_size = 8 ,\n","                        fig_y_size = 6,\n","                        fig_rows = 1 ,\n","                        fig_cols = 2 ,\n","                        linewidth = 0.5 ,\n","                        density = True,\n","                        stacked = True\n","                      )\n"]},{"cell_type":"markdown","metadata":{},"source":["**Diagramas de barras**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15272c83-c581-4892-be2f-e2ada3cc4ced","_uuid":"c8c43ce7-aa03-473b-a735-63eb573c2f45","collapsed":false,"id":"gv1rzvpSlzGA","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Diagramas de barras\n","tw_eda.plot_bar(\n","            layout = 'constrained',\n","            rotation = 0 ,\n","            fig_x_size = 8,\n","            fig_y_size = 6,\n","            fig_rows = 1,\n","            fig_cols = 2 ,\n","            plot_limit_categories = 20\n","         )"]},{"cell_type":"markdown","metadata":{},"source":["**Diagramas de barras combinados**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eaac2ade-5ef4-48e6-8da2-4792807a59f8","_uuid":"715be607-b2be-41e3-b4dc-6519733aed0f","collapsed":false,"id":"5wRXT_zRlzGA","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Diagramas de barras combinados\n","tw_eda.plot_combined_bar(\n","                                layout = 'constrained',\n","                                fig_x_size = 8,\n","                                fig_cols = 1,\n","                                value_feature =  'followers',\n","                                target_feature = 'mes',\n","                                max_categories = 10,\n","                                divide_feature = \"label_name\",\n","                                errorbar = None,\n","                                estimator = np.sum, # Estimator puede ser un callable (es decir funcion) np.mean, np.sum o 'sum', ... funciones de numpy que se apliquen a un vector\n","                                                    # np.mean te hace la media de todo el vestor y te la plotea en el eje y\n","                                                    # np.sum te suma todo el vector y te lo plotea en el eje y\n","                                color = 0,\n","                                )"]},{"cell_type":"markdown","metadata":{},"source":["**Se observa que solo hay un mes con ambas etiquetas, esto será relevante mas adelante y se comentará como se puede aprovechar esta situación**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Comprobamos si solo hay NEGATIVES en dec y jan o es por la escala de la grafica\n","tw_eda.data[\"mes\"][tw_eda.data[\"label_name\"] == \"NEGATIVE\"].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Contamos solo registros de jan y dec\n","df_solo_jan_dec = tw_eda.data[(tw_eda.data[\"nombre_mes\"] == 'January') | (tw_eda.data[\"nombre_mes\"] == 'December')]\n","df_solo_jan_dec.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Comprobamos en que meses solo hay POSITIVES\n","tw_eda.data[\"mes\"][tw_eda.data[\"label_name\"] == \"POSITIVE\"].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Contamos los registros de los demas meses\n","tw_eda.data.shape[0] - df_solo_jan_dec.shape[0] "]},{"cell_type":"markdown","metadata":{},"source":["**Efectivamente, solo en diciembre y enero hay tweets etiquetados como NEGATIVE. Luego, se va a proponer un modelo solo para estos meses y otro solo para los demás meses. Esto se explicará con más detalle después, pero consistirá en que un dataset para entrenar el modelo solo contendrá tweets de estos meses, y en los demás meses se utilizará un modelo dummy que solo prediga la única etiqueta de estos meses**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Diagramas de barras combinados\n","# Miramos si tambien sucede para el año del post del tweet al igual que con el mes\n","tw_eda.plot_combined_bar(\n","                                layout = 'constrained',\n","                                fig_x_size = 8,\n","                                fig_cols = 1,\n","                                value_feature =  'followers',\n","                                target_feature = 'año',\n","                                max_categories = 10,\n","                                divide_feature = \"label_name\",\n","                                errorbar = None,\n","                                estimator = np.mean, # Estimator puede ser un callable (es decir funcion) np.mean, np.sum o 'sum', ... funciones de numpy que se apliquen a un vector\n","                                                    # np.mean te hace la media de todo el vestor y te la plotea en el eje y\n","                                                    # np.sum te suma todo el vector y te lo plotea en el eje y\n","                                color = 0,\n","                                )"]},{"cell_type":"markdown","metadata":{},"source":["**El resultado es similar pero no tan claro, ademas no sirve de mucho saber que en el pasado fue de esta forma pero si en que meses se twitean mas tweets de cierto sentimiento. Porque se puede extrapolar a cualquier año.**"]},{"cell_type":"markdown","metadata":{},"source":["**Diagramas de dispersión con deteccion de outliers [valor > 15 veces el rango intercuartílico para esa feature]**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Scatter plot contra labels\n","tw_eda.plot_scatter(\n","                            fig_x_size = 12,\n","                            fig_cols = 2,\n","                            linewidth =  0.5,\n","                            layout = 'constrained' ,\n","                            x =   ['followers','friends', 'favourites', 'statuses', 'retweets'],\n","                            y = [\"friends\"],\n","                            size = \"retweets\",\n","                            hue = \"label\",\n","                            color = 2,\n","                            plotting_lib = 'seaborn',\n","                            umbral = 15,\n","                            show_outliers = True,\n","                            plotly_opacity = 0.6,\n","                            plotly_colorscale = 20, #[19,20,21,22,23,24]\n","                            plotly_bgcolor = None ,# str or none\n","                            save_figure = None, #[\"jpeg\",\"png\",\"WebP\",None]\n","                            name_figure = \"Fig\"\n","                            )"]},{"cell_type":"markdown","metadata":{},"source":["**Mapas de calor de la matriz de correlación [calculo del coeficiente de pearson para cada par de variables/features numéricas]**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfbaf99a-fe41-4003-94d3-b1e76df9af57","_uuid":"51aa5deb-cab5-4570-b3ab-39647a3a1baf","collapsed":false,"id":"2n0c6A2SlzGA","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# mapa de calor de la matriz de correlacion (coeficientes de pearson entre cada par de features numericas)\n","tw_eda.plot_corr_heatmap(\n","                                fig_x_size = 10,\n","                                fig_y_size = 8,\n","                                layout = 'constrained' ,\n","                                linewidth = 0.5,\n","                                cmap = 'coolwarm', # 'Set1', 'Set2', 'Set3', 'Paired','Accent', 'Prism', 'Dark2','Paired', 'coolwarm', 'viridis', 'cubehelix'\n","                                annot = True, # If True, plot el valor numerico de corrrelacion (entre variables) en la celda del mapa de calor\n","                                fmt = '.2f' # Especificacion del formato numerico\n","\n","                                )"]},{"cell_type":"markdown","metadata":{},"source":["**Se observa que el caso del dataset sin estandarizar ni dropear columnas, la variable target esta negativamente relacionada con el id del tweet, los tweets clasificados como 1 tendran menores valores del id del post**"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b35f7d69-ff85-4f60-a568-e445129d451f","_uuid":"8d834ca5-9d90-451f-ade1-9dcda2ae2cdf","id":"H3x9rUF3lzGA","trusted":true},"source":["**Pero, apenas hay correlacion lineal, excepto por el numero de followers y el numero de friends que estan positivamente realcionadas, si una aumenta la otra tambien. Podria prescindir de una de ellas para la prediccion. Tambien deberiamos investigar entre cada par de variables numericas otro tipo de relaciones no lineales usando por ejemplo metdos basados en arboles de decison como el metodo: Predictive power score**\n"]},{"cell_type":"markdown","metadata":{},"source":["**Predictive power score:**"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8caba3f1-959b-4221-a728-9f1ea19d207c","_uuid":"46697330-ad52-47fa-b7a8-e806f1784ace","collapsed":false,"id":"KeHJXzrwlzGB","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Predictive power score\n","predictors_df = pps.predictors(df, \"label_name\")\n","predictors_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dropeamos los tweets porque es un calculo muy pesado para la matriz de pps \n","df_no_text = df.drop(columns=[\"post_text\"], inplace=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualizacion de los pps predictors: [incluyen los tweets]\n","plt.figure(figsize=(12,6))\n","sns.barplot(data=predictors_df, x=\"x\", y=\"ppscore\", orient=\"v\", color =\"red\")\n","plt.xticks(rotation=45, ha='right')\n","plt.grid()\n","plt.title(\"PPS for features\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dropeamos los tweets porque es un calculo muy pesado para la matriz de pps \n","df_no_text = df.drop(columns=[\"post_text\"], inplace=False)\n","# calculate matrix \n","matrix_df = pps.matrix(df_no_text)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualizacion de la pps matrix:\n","plt.figure(figsize=(16,12))\n","sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)\n","plt.title(\"Matrix PPS\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Se confirma que el año y el mes, por ejemplo, son features potentes para la predicción basándose en modelos de tipo árbol de decisión. Esto es porque ambas ayudan en la división de los nodos de este tipo de algoritmos, dividiendo de manera eficiente en las dos clases objetivo mejor que otras features. Luego, tiene sentido usar un modelo para ciertos meses y otro para otros** "]},{"cell_type":"markdown","metadata":{"_cell_guid":"95796d50-2c67-4091-a859-fc52a1a0b588","_uuid":"c8fd6a79-d1a8-43d8-84df-01e453421bc4","id":"_B27IjxllzGE","trusted":true},"source":["## **Preprocesamiento**"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a0a99c89-40f0-4c5f-9a57-8555ff50e7df","_uuid":"1d468491-e042-4f2d-9fc3-1b78929d1af7","id":"QwotRRHZlzGE","trusted":true},"source":["**Duplicados**"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8bd5e311-d208-478e-af32-ae24bfd29b03","_uuid":"ddbd66a0-0df6-45a6-851a-b22e2de8358d","id":"le6GJtfKlzGE","trusted":true},"source":["Numero de registros duplicados"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c7fbb9b-ba06-4171-9ab4-fe7cfc39a44b","_uuid":"499fa7a4-3c5c-4d0a-a797-a2814a6519c6","collapsed":false,"executionInfo":{"elapsed":339,"status":"ok","timestamp":1716907766397,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"yVvhkA3wlzGE","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Numero de registros duplicados : mismoss valores de cada campo en toda la fila == son el mismo registro -> drop de esas filas\n","duplicated_registers = tw_eda.data.duplicated()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"742fcebf-91b0-4e28-a559-11e796b8ca1a","_uuid":"4b6e1b23-cbc9-4490-a6b4-94504014c530","collapsed":false,"executionInfo":{"elapsed":336,"status":"ok","timestamp":1716907767132,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"jvBIBjbVlzGE","jupyter":{"outputs_hidden":false},"outputId":"79a4f8cd-de10-48a5-9289-a40b9cc2fa3d","trusted":true},"outputs":[],"source":["duplicated_registers.sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60910897-e049-4b8a-8cc3-abe1404e92ab","_uuid":"9d89b348-7e9d-4e6f-8335-d79e0098485d","collapsed":false,"executionInfo":{"elapsed":249,"status":"ok","timestamp":1716907770445,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"8UibGdbPlzGE","jupyter":{"outputs_hidden":false},"outputId":"24d63b4a-fc12-4ab9-871b-9062188d9572","trusted":true},"outputs":[],"source":["duplicated_registers_df = tw_eda.data[duplicated_registers]\n","duplicated_registers_df.head()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2e8b4bef-7332-41ab-b352-490c746bcd1c","_uuid":"449536aa-7a4d-4799-947b-a17a2f9b9a7e","id":"KDLDmcF7lzGF","trusted":true},"source":["Hay 117 filas iguales, se podrian eliminar pero no se va a realizar en este caso, dado que podrian tratarse de reiteraciones de u na persona expresando el mismo sentimiento, o de varias perosnas expresando el mismo sentimeinto y creo que es beneficioso para el entrenamiento del modelo"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2130fd38-bf76-4ad7-aab6-afc3f3d57bdd","_uuid":"29bf76a6-c412-4015-8d5c-1aa9e9a48ac8","id":"K-r9v_9QlzGF","trusted":true},"source":["Número de tweets iguales"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26cd1af6-c43e-4b2d-91f8-c70aa7acefe3","_uuid":"3b9d4232-5c62-454c-8f5a-972b0d17b068","collapsed":false,"executionInfo":{"elapsed":407,"status":"ok","timestamp":1716907780647,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"9xoDwJTKlzGF","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["duplicated_texts = tw_eda.data.duplicated(subset=['post_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0c4c296-9307-4242-92c4-127712f45328","_uuid":"1abe2a67-b653-4ce4-989a-dcd89e442a78","collapsed":false,"executionInfo":{"elapsed":362,"status":"ok","timestamp":1716907782193,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"AyCpBMNKlzGF","jupyter":{"outputs_hidden":false},"outputId":"ec840e0d-7487-4b92-ad05-97a38fcacf42","trusted":true},"outputs":[],"source":["duplicated_texts.sum()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"1176ba8e-3d32-41ee-ad71-14f0f6161c67","_uuid":"95eeac87-b3f2-47ea-a353-e73636cc105e","id":"m5404Z9hlzGF","trusted":true},"source":["Hay 512 tweets iguales, se proceden a evaluar sus demas campos: ids, fecha post,..."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"342d1327-6fdf-4575-bdd9-a32802bc7b25","_uuid":"9fd0f422-81bf-4578-9c01-749e645d84b1","collapsed":false,"executionInfo":{"elapsed":403,"status":"error","timestamp":1716907784108,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"dONHOxRXlzGF","jupyter":{"outputs_hidden":false},"outputId":"f4e4f849-38c1-4606-8151-0a4f95a12cf9","trusted":true},"outputs":[],"source":["duplicate_text_df = tw_eda.data[duplicated_texts]\n","tw_duplicated_eda = Eda(data =duplicate_text_df)\n","tw_duplicated_eda.data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4302e5d-4235-4463-8dc0-5b824bd5f566","_uuid":"bcba9fe0-255d-410b-bc1f-2cd40c0a3e29","collapsed":false,"id":"lv_1DMAilzGF","jupyter":{"outputs_hidden":false},"outputId":"6276a1fb-a840-4ebf-853c-443dd197fd17","trusted":true},"outputs":[],"source":["# numero de user_id diferentes en los twweets repetidos\n","tw_duplicated_eda.data['user_id'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15bf5474-6d50-49cf-ba72-5119d10fb791","_uuid":"5041cabd-2d1f-44e6-abfd-40b010ee471c","collapsed":false,"id":"ZymCLYculzGF","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# textos repetidos\n","tw_duplicated_eda.data['post_text'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a33ccf7d-7c3b-4f85-951b-70a0351e85af","_uuid":"031dba67-0ff7-4866-ae41-f1a460a7870f","collapsed":false,"id":"AQG45xxLlzGF","jupyter":{"outputs_hidden":false},"outputId":"51e86b69-d558-4b34-c446-1dd6d4ce9dae","trusted":true},"outputs":[],"source":["# user_id [numerica] -> user_id_cat [categorica]\n","import string\n","uppercase_letters = list(string.ascii_uppercase)  # lista de letras de 'A' a 'Z' usando el módulo string\n","print(uppercase_letters)\n","map_user = {user_id:f'user_{letter}' for letter,user_id in zip(uppercase_letters,tw_duplicated_eda.data['user_id'].unique())}\n","tw_duplicated_eda.data['user_id_cat'] = tw_duplicated_eda.data['user_id'].map(map_user)\n","tw_duplicated_eda = Eda(data=tw_duplicated_eda.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d34e45b-4003-4ba9-88d9-5a1e7161f644","_uuid":"e3b375ff-f3c3-49f2-8265-30bfd1bf4c26","collapsed":false,"id":"1zyDTbLulzGF","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Diagramas de barras\n","tw_duplicated_eda.plot_bar(\n","            layout = 'constrained',\n","            rotation = 0 ,\n","            fig_x_size = 16,\n","            fig_y_size = 6,\n","            fig_rows = 1,\n","            fig_cols = 2 ,\n","            plot_limit_categories = 20\n","         )"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5d991be-a794-4a5a-8ac7-b89989c8e127","_uuid":"c02c1e82-3ea1-4a1b-84e7-99e57f6bc5b7","collapsed":false,"id":"ir4M7RyBlzGG","jupyter":{"outputs_hidden":false},"outputId":"5bbd8a3b-2108-4ba0-d99d-380067ff2728","trusted":true},"outputs":[],"source":["user_h = tw_duplicated_eda.data[tw_duplicated_eda.data['user_id_cat'] == 'user_H']\n","print(user_h['post_text'].head(10))"]},{"cell_type":"markdown","metadata":{},"source":["### **Clase preprocesado de texto : lematizacion, eliminacion stopwords, urls, emojis, BOW, BOW con TF-IDF ...**"]},{"cell_type":"markdown","metadata":{},"source":["Con esta clase se van a probar y aplicar una serie de métodos de limpieza/preprocesamiento para aplicar BOW y BOW corregido con TF-IDF para ver la frecuencia en los textos de ciertos términos, basándose en cuántas veces aparecen en cada documento o texto y ponderando por el IDF. Ambos métodos se podrían usar [aunque no se ha hecho] posteriormente para identificar estos tokens o términos más o menos frecuentes y eliminarlos según se observe si son o no relevantes para el problema de clasificación de sentimientos en cuestión que se está tratando. Después de esto, se van a obtener diferentes datasets para ver cómo se comporta el modelo en función del preprocesamiento que cada uno haya tenido."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dataset solo con enero y diciembre\n","df_solo_jan_dec = df[(df[\"nombre_mes\"] == 'January') | (df[\"nombre_mes\"] == 'December')]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@dataclass\n","class TextPreprocess:\n","    \"\"\"Class for text preprocess\"\"\"\n","    patron_emoji  : ClassVar = re.compile(\n","                                            \"[\"\n","                                            \"\\U0001F600-\\U0001F64F\"  # Emoticons\n","                                            \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n","                                            \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n","                                            \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n","                                            \"\\U00002700-\\U000027BF\"  # Dingbats\n","                                            \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","                                            \"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n","                                            \"\\U00002B50-\\U00002B55\"  # Additional symbols\n","                                            \"\\U00002300-\\U000023FF\"  # Miscellaneous Technical\n","                                            \"\\U0000200D\"             # Zero Width Joiner\n","                                            \"\\U00002500-\\U000025FF\"  # Geometric Shapes\n","                                            \"\\U00002100-\\U0000219F\"  # Arrows\n","                                            \"]+\",\n","                                            flags=re.UNICODE,\n","                                        )\n","    \n","    patron_chinese_japanese: ClassVar = re.compile(\n","        r'[\\u4e00-\\u9fff]|'  # Basic Chinese\n","        r'[\\u3400-\\u4dbf]|'  # Extended Chinese\n","        r'[\\u3040-\\u309f]|'  # Hiragana\n","        r'[\\u30a0-\\u30ff]|'  # Katakana\n","        r'[\\uff66-\\uff9f]'   # Half-width Katakana\n","    )\n","    task : str\n","    corpus : List[str]\n","    spc_caracters : Optional[List[str]] = field(default_factory=list)\n","    data : Optional[pd.DataFrame] = None\n","    \n","    def del_stopwords(self, lang: str) -> 'TextPreprocess':\n","        empty_words = set(stopwords.words(lang))\n","        for i, t in enumerate(self.corpus):\n","            self.corpus[i] = ' '.join([word for word in t.split() if word.lower() not in empty_words])\n","        return self\n","\n","    def del_urls(self) -> 'TextPreprocess':\n","        patron_url = re.compile(r'https?://\\S+|www\\.\\S+')\n","        for i, t in enumerate(self.corpus):\n","            processed_text = re.sub(patron_url, '', t)\n","            self.corpus[i] = re.sub(r'\\s+', ' ', processed_text.strip())\n","        return self\n","\n","    def del_html(self) -> 'TextPreprocess':\n","        html_tags_pattern = re.compile(r'<.*?>')\n","        for i, t in enumerate(self.corpus):\n","            processed_text = re.sub(html_tags_pattern, '', t)\n","            self.corpus[i] = re.sub(r'\\s+', ' ', processed_text.strip())\n","        return self\n","\n","    def del_emojis(self) -> 'TextPreprocess':\n","        for i, t in enumerate(self.corpus):\n","            processed_text = re.sub(TextPreprocess.patron_emoji, '', t)\n","            self.corpus[i] = re.sub(r'\\s+', ' ', processed_text.strip())\n","        return self\n","    \n","    def del_special(self) -> 'TextPreprocess':\n","        for i, t in enumerate(self.corpus):\n","            self.corpus[i] = ''.join([c for c in t if c != self.spc_caracters])\n","        return self\n","\n","    def del_digits(self) -> 'TextPreprocess':\n","        for i, t in enumerate(self.corpus):\n","            processed_text = re.sub(r'[0-9]+', '', t)\n","            self.corpus[i] = re.sub(r'\\s+', ' ', processed_text.strip())\n","        return self\n","    \n","    def del_chinese_japanese(self) -> 'TextPreprocess':\n","        for i, t in enumerate(self.corpus):\n","            processed_text = re.sub(TextPreprocess.patron_chinese_japanese, '', t)\n","            self.corpus[i] = re.sub(r'\\s+', ' ', processed_text.strip())\n","        return self\n","    \n","    def del_extra_spaces(self) -> 'TextPreprocess':\n","        for i, t in enumerate(self.corpus):\n","            self.corpus[i] = re.sub(r'\\s+', ' ', t.strip())\n","        return self\n","\n","    def get_lower(self) -> 'TextPreprocess':\n","        for i, t in enumerate(self.corpus):\n","            self.corpus[i] = t.lower()\n","        return self\n","\n","    def get_alfanumeric(self) -> 'TextPreprocess':\n","        for i, t in enumerate(self.corpus):\n","            processed_text = re.sub(r'[^\\w\\s]|_', '', t)\n","            self.corpus[i] = re.sub(r'\\s+', ' ', processed_text.strip())\n","        return self\n","\n","    def stem(self) -> 'TextPreprocess':\n","        Porter = PorterStemmer()\n","        for i, t in enumerate(self.corpus):\n","            word_tokens = t.split()\n","            stems = [Porter.stem(word) for word in word_tokens]\n","            self.corpus[i] = ' '.join(stems)\n","        return self\n","\n","    def lemmatize(self) -> 'TextPreprocess':\n","        lemmatizer = WordNetLemmatizer()\n","        for i, t in enumerate(self.corpus):\n","            word_tokens = t.split()\n","            lemmas = [lemmatizer.lemmatize(word, pos='v') for word in word_tokens]\n","            self.corpus[i] = ' '.join(lemmas)\n","        return self\n","    \n","    def custom_del(\n","                self,\n","                text_field_name: str,\n","                special_c: Optional[List[str]] = None,\n","                data: Optional[Union[pd.DataFrame,str]] = None,\n","                delete: bool = False,\n","                plot: bool = False\n","                ) -> Tuple[dict, pd.DataFrame]: \n","        \n","        \"\"\"Method for custom preprocess/delete chaaracters ferom List[texts] or text (string)\"\"\"\n","        if data is None:\n","            data = self.data\n","        if special_c is None:\n","            special_c = self.spc_caracters\n","        \n","        if data is None:\n","            raise ValueError(\"Data must be provided either as a class attribute or as a method parameter.\")\n","        if special_c is None:\n","            raise ValueError(\"Special characters list must be provided either as a class attribute or as a method parameter.\")\n","        \n","        if isinstance(data, pd.DataFrame):\n","            data_is_string = False\n","            df = data.copy().reset_index(drop=True)  # Reset index here\n","        elif isinstance(data, str):\n","            data_is_string = True\n","            text = data\n","            df = pd.DataFrame()\n","        else:\n","            raise ValueError(\"Unknown non-process-type of 'data' parameter\")\n","\n","        \n","        special_c_count = {}\n","        if not text_field_name:\n","            raise ValueError(\"text_field_name must be defined\")\n","        \n","        for char in special_c:\n","            count = 0\n","            patron_busqueda = re.compile(re.escape(char))\n","            if data_is_string:\n","                match_obj = patron_busqueda.search(text)\n","                if char in text or match_obj is not None:\n","                    count += 1\n","                    if delete:\n","                        text = ''.join([c for c in text if c != char])\n","                        #text = re.sub(patron_busqueda, '', text) # alternative method\n","                special_c_count[char] = count\n","            else:\n","                for i in range(df.shape[0]):\n","                        text = df.loc[i, text_field_name]\n","                        match_obj = patron_busqueda.search(text)\n","                        if char in text or match_obj is not None:\n","                            count += 1\n","                            if delete:\n","                                df.loc[i, text_field_name] = ''.join([c for c in text if c != char])\n","                                #df.loc[i, text_field_name] = re.sub(patron_busqueda, '', text) # alternative method\n","                        special_c_count[char] = count\n","                \n","        \n","        if plot:\n","            plt.figure(figsize=(10, 6))\n","            plt.bar(special_c_count.keys(), special_c_count.values(), color='skyblue')\n","            plt.xlabel('Special Characters')\n","            plt.ylabel('Frequency')\n","            plt.title('Special Characters in Texts')\n","            plt.xticks(rotation=45)\n","            plt.grid()\n","            plt.show()\n","            \n","        if data_is_string:\n","            return special_c_count, text\n","        else:\n","            return special_c_count, df\n","        \n","    def bow(self) -> pd.DataFrame:\n","        vectorizador = CountVectorizer(\n","            input='content',\n","            encoding='utf-8',\n","            decode_error='strict',\n","            strip_accents=None,\n","            lowercase=True,\n","            preprocessor=None,\n","            tokenizer=None,\n","            stop_words=None,\n","            token_pattern=r'(?u)\\b\\w\\w+\\b',\n","            ngram_range=(1, 1),\n","            analyzer='word',\n","            max_df=1.0,\n","            min_df=1,\n","            max_features=None,\n","            vocabulary=None,\n","            binary=False\n","        )\n","\n","        try:\n","            X = vectorizador.fit_transform(self.corpus)\n","        except UnicodeDecodeError as e:\n","            print(f\"Error: characters not of the given encoding -> {e}\")\n","            return pd.DataFrame()\n","\n","        nombres_caracteristicas = vectorizador.get_feature_names_out()\n","        return pd.DataFrame(data=X.toarray(), columns=nombres_caracteristicas, index=self.corpus)\n","\n","    def bow_tf_idf(self) -> pd.DataFrame:\n","        tfidf_vectorizador = TfidfVectorizer(\n","            input='content',\n","            encoding='utf-8',\n","            decode_error='strict',\n","            strip_accents=None,\n","            lowercase=True,\n","            preprocessor=None,\n","            tokenizer=None,\n","            analyzer='word',\n","            stop_words=None,\n","            token_pattern=r'(?u)\\b\\w\\w+\\b',\n","            ngram_range=(1, 1),\n","            max_df=1.0,\n","            min_df=1,\n","            max_features=None,\n","            vocabulary=None,\n","            binary=False,\n","            dtype=np.float64,\n","            norm='l2',\n","            use_idf=True,\n","            smooth_idf=True,\n","            sublinear_tf=False\n","        )\n","\n","        try:\n","            X = tfidf_vectorizador.fit_transform(self.corpus)\n","        except UnicodeDecodeError as e:\n","            print(f\"Error: characters not of the given encoding -> {e}\")\n","            return pd.DataFrame()\n","\n","        terms = tfidf_vectorizador.get_feature_names_out()\n","        return pd.DataFrame(data=X.toarray(), columns=terms, index=self.corpus)\n","\n","    \n","\n","# df original\n","preprocessor_bow = TextPreprocess(\n","    task='classification task ',\n","    corpus=df[\"post_text\"].tolist(),\n","    spc_caracters = [\n","                        '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '=', '+',\n","                        '{', '}', '[', ']', '|', '\\\\', ':', ';', '\"', \"'\", '<', '>', ',', '.',\n","                        '?', '/', '~', '`', '\\n', '\\r', '\\t', '\\b', '\\f','__'\n","                    ]\n",")\n","# df original\n","preprocessor_bow_tf_idf = TextPreprocess(\n","    task='classification task ',\n","    corpus=df[\"post_text\"].tolist(),\n","    spc_caracters = [\n","                        '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '=', '+',\n","                        '{', '}', '[', ']', '|', '\\\\', ':', ';', '\"', \"'\", '<', '>', ',', '.',\n","                        '?', '/', '~', '`', '\\n', '\\r', '\\t', '\\b', '\\f','__'\n","                    ]\n",")\n","\n","# Solo meses enero y diciemnbre\n","preprocessor_bow_tf_idf_jan_dec = TextPreprocess(\n","    task='classification task ',\n","    corpus=df_solo_jan_dec[\"post_text\"].tolist(),\n","    spc_caracters = [\n","                        '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '=', '+',\n","                        '{', '}', '[', ']', '|', '\\\\', ':', ';', '\"', \"'\", '<', '>', ',', '.',\n","                        '?', '/', '~', '`', '\\n', '\\r', '\\t', '\\b', '\\f','__'\n","                    ]\n",")\n","\n","bow_df = preprocessor_bow \\\n","    .del_urls() \\\n","    .get_lower() \\\n","    .del_chinese_japanese() \\\n","    .get_alfanumeric() \\\n","    .del_digits() \\\n","    .del_emojis() \\\n","    .del_special() \\\n","    .bow()\n","\n","\n","bow_df_tf_idf = preprocessor_bow_tf_idf \\\n","    .del_urls() \\\n","    .del_emojis() \\\n","    .get_lower() \\\n","    .del_chinese_japanese() \\\n","    .get_alfanumeric() \\\n","    .del_digits() \\\n","    .del_special() \\\n","    .bow_tf_idf()\n","    \n","bow_df_tf_idf_jan_dec = preprocessor_bow_tf_idf_jan_dec \\\n","    .del_urls() \\\n","    .del_emojis() \\\n","    .get_lower() \\\n","    .del_chinese_japanese() \\\n","    .get_alfanumeric() \\\n","    .del_digits() \\\n","    .del_special() \\\n","    .bow_tf_idf()"]},{"cell_type":"markdown","metadata":{},"source":["**Plotean y muestran los resultados del BOW y BOW con TF-IDF. Después se asignan a tres datasets diferentes para entrenar el modelo.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check what are the tokens/terms most frequent in the corpus according the two methods [sum of the df columns -> (row,column) : (doc,term)]\n","most_frequent_bow = bow_df.sum(axis=0, skipna=True).sort_values(ascending=False)\n","most_frequent_bow_tf_idf = bow_df_tf_idf.sum(axis=0, skipna=True).sort_values(ascending=False)\n","most_frequent_bow_tf_idf_jan_dec = bow_df_tf_idf_jan_dec.sum(axis=0, skipna=True).sort_values(ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_tokens=50\n","plt.figure(figsize=(16,10))\n","plt.bar(x=most_frequent_bow.head(num_tokens).index, height=most_frequent_bow.head(num_tokens).values)\n","plt.xticks(rotation=45, ha='right')\n","plt.title(f\"Most frequent {num_tokens} tokens/terms in corpus using bow raw method\")\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_tokens=50\n","plt.figure(figsize=(16,10))\n","plt.bar(x=most_frequent_bow_tf_idf.head(num_tokens).index, height=most_frequent_bow_tf_idf.head(num_tokens).values)\n","plt.xticks(rotation=45, ha='right')\n","plt.title(f\"Most frequent {num_tokens} tokens/terms in corpus using bow_tf_idf method\")\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_tokens=50\n","plt.figure(figsize=(16,10))\n","plt.bar(x=most_frequent_bow_tf_idf_jan_dec.head(num_tokens).index, height=most_frequent_bow_tf_idf_jan_dec.head(num_tokens).values)\n","plt.xticks(rotation=45, ha='right')\n","plt.title(f\"Most frequent {num_tokens} tokens/terms in corpus [tweets posted in January/and December only] using bow_tf_idf method\")\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_tokens=20\n","plt.figure(figsize=(12,6))\n","plt.bar(x=most_frequent_bow.tail(num_tokens).index, height=most_frequent_bow.tail(num_tokens).values)\n","plt.xticks(rotation=45, ha='right')\n","plt.title(f\"Less frequent {num_tokens} tokens/terms in corpus using bow raw method\")\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_tokens=20\n","plt.figure(figsize=(12,6))\n","plt.bar(x=most_frequent_bow_tf_idf.tail(num_tokens).index, height=most_frequent_bow_tf_idf.tail(num_tokens).values)\n","plt.xticks(rotation=40, ha='right')\n","plt.title(f\"Less frequent {num_tokens} tokens/terms in corpus using bow_tf_idf method\")\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_tokens=20\n","plt.figure(figsize=(12,6))\n","plt.bar(x=most_frequent_bow_tf_idf_jan_dec.tail(num_tokens).index, height=most_frequent_bow_tf_idf_jan_dec.tail(num_tokens).values)\n","plt.xticks(rotation=40, ha='right')\n","plt.title(f\"Less frequent {num_tokens} tokens/terms in corpus  [tweets posted in January/and December only] using bow_tf_idf method\")\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**A continuación se exploran y plotean los caracteres especiales existentes en el texto original y su frecuencia [esto se han eliminado de los tres datasets, una vía de investigación y sobretodo con modelo de tipo DEBERTA sería no eliminarlos para ver si ayudan a mejorar la predicción]**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Exploring and visualization of special caracters\n","preprocessor_special_only= TextPreprocess(\n","    task='classification task ',\n","    corpus=df[\"post_text\"].tolist(),\n","    data = df,\n","    spc_caracters = [\n","                        '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '=', '+',\n","                        '{', '}', '[', ']', '|', '\\\\', ':', ';', '\"', \"'\", '<', '>', ',', '.',\n","                        '?', '/', '~', '`', '\\n', '\\r', '\\t', '\\b', '\\f','__'\n","                    ]\n",")\n","\n","df_box_tf_idf = preprocessor_special_only.custom_del(\n","                                                        text_field_name ='post_text',\n","                                                        delete=False,\n","                                                        plot= True\n","                                                    )"]},{"cell_type":"markdown","metadata":{},"source":["## **INTENTO DE CLASIFICACION CON MODELO CON DEBERTA, USO DE LIBRERIA TRANSFORMERS Y DE LIBERRIA DATASETS DE HG**"]},{"cell_type":"markdown","metadata":{"_cell_guid":"6768b16a-a799-4d64-af23-1548e574779e","_uuid":"2b0f7891-f93c-4e55-ab43-fa7102a7967d","id":"AqVY2-ZmLHU1","trusted":true},"source":["### Creacion dataset de entrenamiento [HG Dataset]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82855957-1726-49c5-9c12-1396b0b1bd23","_uuid":"d709370f-8591-4195-9b27-a7ab7ddb94da","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:54:10.956173Z","iopub.status.busy":"2024-05-29T07:54:10.955812Z","iopub.status.idle":"2024-05-29T07:54:10.960705Z","shell.execute_reply":"2024-05-29T07:54:10.959733Z","shell.execute_reply.started":"2024-05-29T07:54:10.956146Z"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1716911282436,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"BQBquAipfTJQ","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# util functions\n","def get_current_utc_date_iso():\n","    # Get the current date and time in UTC and format it directly\n","    return datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79805d6f-a77a-4681-8717-bb288af87feb","_uuid":"365a0bad-a8a8-48d2-bcfa-b7de4b536e27","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T07:54:11.180969Z","iopub.status.busy":"2024-05-29T07:54:11.180645Z","iopub.status.idle":"2024-05-29T07:54:11.221977Z","shell.execute_reply":"2024-05-29T07:54:11.221104Z","shell.execute_reply.started":"2024-05-29T07:54:11.180944Z"},"executionInfo":{"elapsed":883,"status":"ok","timestamp":1716911283761,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"1cHcuoihLM34","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class TwDataset:\n","  def __init__(self, tokenizer,padding, truncate, max_position_embeddings, df, text_field_name, label_field_name, id_field_name):\n","    self.tokenizer = tokenizer\n","    self.padding = padding\n","    self.truncate = truncate\n","    self.max_length = max_position_embeddings\n","    self.data = df\n","    try:\n","      self.labels = df[label_field_name].unique()\n","    except Exception as e:\n","      print(e)\n","    self.label2id = {l:i for i,l in enumerate(self.labels)}\n","    self.id2label = {i:l for i,l in enumerate(self.labels)}\n","    self.text_f_name = text_field_name\n","    self.label_f_name = label_field_name\n","    if id_field_name:\n","      self.id_field_name = id_field_name\n","      self.ids = {k:[] for k in id_field_name}\n","      print('ID FIELDS :',self.ids)\n","    self.tokens = []\n","    self.len_texts = []\n","\n","\n","  def _tokenize_texts(self, texts : List[str], tensor :bool = False):\n","    if isinstance(texts , list):\n","      if self.tokenizer is not None:\n","        if tensor:\n","          x = self.tokenizer(texts,  padding=self.padding, truncation=self.truncate,  max_length =self.max_length, return_tensors=\"pt\")\n","        else:\n","          x = self.tokenizer(texts, padding=self.padding, truncation=self.truncate, max_length =self.max_length)\n","        return x\n","      else:\n","        raise ValueError('No tokenizer passed as argument')\n","\n","\n","  def _process_dataset(self, dataset) -> dict:\n","      text_name_field = self.text_f_name if isinstance(self.text_f_name, str) else None\n","      if text_name_field is None:\n","          raise AttributeError(\"Text field name must be str\")\n","\n","      text = str(dataset[text_name_field]) # aseguramos tipo de dato es str\n","\n","      # tokenizacion\n","      #tokenized = self.tokenizer(text, padding=True, truncation=True, ,max_lenght =512 )\n","      tokenized = self.tokenizer(text,padding=self.padding, truncation=self.truncate, max_length =self.max_length)\n","\n","      # calculo de tokens , caracteres y ids por texto\n","      self.tokens.append(len(tokenized[\"input_ids\"]))\n","      self.len_texts.append(len(text))\n","      for id_i in self.ids.keys():\n","        self.ids[id_i].append(dataset[id_i])\n","\n","      # labels\n","      tokenized[\"labels\"] = dataset[self.label_f_name]\n","\n","      return tokenized\n","\n","\n","  def get_hg_dataset(self, split : bool = False, tokenize :bool = True, from_csv: bool =False, csv_path :str = None ):\n","\n","      # load original dataset from path\n","      if from_csv:\n","        try:\n","          dataset = load_dataset(\"csv\", data_files=csv_path, split = 'train')\n","          try:\n","            dataset = dataset.remove_columns([\"Unnamed: 0\"])\n","          except:\n","            pass\n","          print(dataset)\n","        except Exception as e:\n","            print(e)\n","        if split:\n","          dataset_train_test = dataset.train_test_split(test_size = 0.2, shuffle = True, seed =42)\n","          dataset_train_val = dataset_train_test[\"train\"].train_test_split(test_size = 0.1, shuffle = True, seed =42)\n","          dataset = DatasetDict({\n","                                                  \"train\": dataset_train_val[\"train\"],\n","                                                  \"validation\": dataset_train_val[\"test\"],\n","                                                  \"test\":dataset_train_test[\"test\"]\n","                                                  })\n","      else:\n","        try:\n","            if split:\n","                # test 20 % train\n","                df_train_val, df_test = train_test_split(self.data, test_size=0.2, random_state=42)\n","                print('\\n---------------------------------------------------')\n","                print(\"Test df shape : \", df_test.shape)\n","                # Validation 10% de train\n","                df_train, df_val = train_test_split(df_train_val, test_size=0.1, random_state=42)\n","                print(\"Train df shape : \", df_train.shape)\n","                print(\"validation df shape : \", df_val.shape)\n","                print('---------------------------------------------------\\n')\n","\n","\n","                dataset = DatasetDict({\n","                                                    \"train\": ds.Dataset.from_pandas(df_train, preserve_index=False),\n","                                                    \"validation\": ds.Dataset.from_pandas(df_val, preserve_index=False),\n","                                                    \"test\":ds.Dataset.from_pandas(df_test, preserve_index=False)\n","                                                    })\n","            else:\n","                dataset = DatasetDict({\n","                                    \"train\": ds.Dataset.from_pandas(self.data, preserve_index=False)\n","                                    })\n","\n","\n","            print(\"\\nHG DATASET :\\n \", dataset)\n","            #dataset = load_dataset(self.path).remove_columns([\"Unnamed: 0\"])\n","        except Exception as e:\n","            print(e)\n","\n","\n","      if tokenize:\n","          dataset_tokenize = dataset.map(self._process_dataset, batched=False, remove_columns=self.data.columns.tolist())\n","      else:\n","          dataset_tokenize = None\n","      print(\"\\nHG DATASET TOKENIZE:\\n \", dataset_tokenize)\n","      return dataset,dataset_tokenize\n","\n","  def get_plots(self, dir_path: str, figure_name: str, show : bool = False):\n","\n","      def addlabels(x, y, text, size, rotation):\n","          colors = ['g', 'r', 'c', 'm', 'y', 'k']  # Removed 'b' (blue) from the list\n","          c = 0\n","          for i in range(len(x)):\n","              if c >= len(colors):\n","                  c = 0\n","              plt.annotate(text[i], (x[i], y[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', size=size, color=colors[c], rotation=rotation)\n","              c += 1\n","\n","      # Calculate the number of tokens for each document\n","      if self.tokens and self.len_texts:\n","\n","          # Plotting the histogram of token counts\n","          plt.figure(figsize=(10, 6))\n","          plt.hist(self.tokens, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n","          plt.title(\"Histogram of Token Counts\")\n","          plt.xlabel(\"Token Count\")\n","          plt.ylabel(\"Frequency\")\n","          plt.grid(axis=\"y\", alpha=0.75)\n","          if show:\n","            plt.show()\n","          else:\n","            plot_file = os.path.join(dir_path, get_current_utc_date_iso() + \"_\" + figure_name + \"_\" + \"token_hist.png\")\n","            plt.savefig(plot_file)\n","            plt.close()\n","\n","          # Plotting the histogram of characters counts\n","          plt.figure(figsize=(10, 6))\n","          plt.hist(self.len_texts, bins=30, color=\"red\", edgecolor=\"black\", alpha=0.7)\n","          plt.title(\"Histogram of Character Counts\")\n","          plt.xlabel(\"Character Count\")\n","          plt.ylabel(\"Frequency\")\n","          plt.grid(axis=\"y\", alpha=0.75)\n","          if show:\n","            plt.show()\n","          else:\n","            plot_file = os.path.join(dir_path, get_current_utc_date_iso() + \"_\" + figure_name + \"_\" + \"character_hist.png\")\n","            plt.savefig(plot_file)\n","            plt.close()\n","\n","          # Plotting ordered chunk vs num tokens\n","          SAMPLE_PLOT_SIZE = min(100, len(self.len_texts))\n","          NUM_CHUNKS = np.arange(SAMPLE_PLOT_SIZE)\n","\n","          plt.figure(figsize=(10, 6))\n","          plt.bar(NUM_CHUNKS, self.tokens[:SAMPLE_PLOT_SIZE], color=\"blue\", alpha=1)\n","          addlabels(NUM_CHUNKS, self.tokens[:SAMPLE_PLOT_SIZE], [str(t) for t in self.tokens[:SAMPLE_PLOT_SIZE]], 10, 0)\n","          for i,id_i in enumerate(self.id_field_name):\n","              plot_id = []\n","              for v in self.ids[id_i][:SAMPLE_PLOT_SIZE]:\n","                  plot_id.append('#'+ str(id_i)+':'+str(v))\n","              addlabels(\n","                          x = NUM_CHUNKS,\n","                          y = np.full(len(self.tokens[:SAMPLE_PLOT_SIZE]), np.max(self.tokens[:SAMPLE_PLOT_SIZE]))*(0.1*(i+1)),\n","                          text = plot_id,\n","                          size = 6,\n","                          rotation = 40)\n","          plt.title(\"Token counts per chunk index\")\n","          plt.xlabel(\"CHUNK index\")\n","          plt.ylabel(\"Token counts\")\n","          plt.grid(axis=\"y\", alpha=0.75)\n","          if show:\n","            plt.show()\n","          else:\n","            plot_file = os.path.join(dir_path, get_current_utc_date_iso() + \"_\" + figure_name + \"_\" + \"num_token_per_chunk.png\")\n","            plt.savefig(plot_file)\n","            plt.close()\n","\n","          plt.figure(figsize=(10, 6))\n","          plt.bar(NUM_CHUNKS, self.len_texts[:SAMPLE_PLOT_SIZE], color=\"red\", alpha=1)\n","          addlabels(NUM_CHUNKS, self.len_texts[:SAMPLE_PLOT_SIZE], [str(t) for t in self.len_texts[:SAMPLE_PLOT_SIZE]], 10, 0)\n","          for i,id_i in enumerate(self.id_field_name):\n","              plot_id = []\n","              for v in self.ids[id_i][:SAMPLE_PLOT_SIZE]:\n","                  plot_id.append('#'+ str(id_i)+':'+str(v))\n","              addlabels(\n","                          x = NUM_CHUNKS,\n","                          y = np.full(len(self.len_texts[:SAMPLE_PLOT_SIZE]), np.max(self.tokens[:SAMPLE_PLOT_SIZE]))*(0.2*(i+1)),\n","                          text = plot_id,\n","                          size = 6,\n","                          rotation = 40)\n","          plt.title(\"Character counts per chunk index\")\n","          plt.xlabel(\"CHUNK index\")\n","          plt.ylabel(\"Character counts\")\n","          plt.grid(axis=\"y\", alpha=0.75)\n","          if show:\n","            plt.show()\n","          else:\n","            plot_file = os.path.join(dir_path, get_current_utc_date_iso() + \"_\" + figure_name + \"_\" + \"num_character_per_chunk.png\")\n","            plt.savefig(plot_file)\n","            plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bfd7758-411d-457f-ac36-fb0d2cdf7bd5","_uuid":"6032ead6-be8a-4089-8f8d-02a38572fd32","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:08:56.044737Z","iopub.status.busy":"2024-05-29T08:08:56.043911Z","iopub.status.idle":"2024-05-29T08:09:04.262501Z","shell.execute_reply":"2024-05-29T08:09:04.261457Z","shell.execute_reply.started":"2024-05-29T08:08:56.044698Z"},"executionInfo":{"elapsed":3109,"status":"ok","timestamp":1716911286868,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"ozYWw16VTDXr","jupyter":{"outputs_hidden":false},"outputId":"4a44941a-101a-457a-a7dc-dcfceb29f00d","trusted":true},"outputs":[],"source":["# MODEL\n","from transformers import AutoModel, AutoModel, AutoConfig\n","from transformers import DebertaV2Config\n","CONFIG_DEBERTA_V2 = DebertaV2Config()\n","\n","#MODEL_NAME = 'microsoft/deberta-v2-xlarge'\n","MODEL_NAME = 'microsoft/deberta-v3-base'\n","\n","\n","CONFIG_DEBERTA_V3 = AutoConfig.from_pretrained(\n","                                        pretrained_model_name_or_path =MODEL_NAME\n","                                        )\n","# Como deberta-v3 no tiene definido la maxima dimension de los embedding input \n","# # y su configuracxion es similar a la de deberta-v2 que si lo tiene definido se lo copiamos ded su config \n","CONFIG_DEBERTA_V3.max_position_embeddings= CONFIG_DEBERTA_V2.max_position_embeddings\n","\n","\"\"\"no efecto en el tokenizer:\n","# Para que se haga efectivo en la confg del tokenizer tambnien hay que añadir una nueva key al modelo con el mismo valor:\n","CONFIG_DEBERTA_V3.max_model_input_sizes =CONFIG_DEBERTA_V2.max_position_embeddings\n","\"\"\"\n","\n","model = AutoModel.from_pretrained(\n","                                        pretrained_model_name_or_path = MODEL_NAME,\n","                                        config = CONFIG_DEBERTA_V3)\n","\n","tokenizer = AutoTokenizer.from_pretrained(  \n","                                          pretrained_model_name_or_path =MODEL_NAME,\n","                                          use_fast=True,\n","                                          model_max_length =CONFIG_DEBERTA_V3.max_position_embeddings\n","                                          )"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac384f27-25c5-4044-b122-11586e292f9a","_uuid":"e4f8a562-a930-4e98-ade2-e44d225ea079","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:09:04.265390Z","iopub.status.busy":"2024-05-29T08:09:04.264699Z","iopub.status.idle":"2024-05-29T08:09:04.272772Z","shell.execute_reply":"2024-05-29T08:09:04.271968Z","shell.execute_reply.started":"2024-05-29T08:09:04.265352Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# prueba tokenizer\n","tokenizer.encode(text = \"holamdkmdkdmewkmdmdkdmdmkdmdkm2d\",return_tensors='pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b3e12b6-9c37-43f6-9a09-481abb8ebe22","_uuid":"735c29ce-2d65-48d3-a24d-627d7557e63b","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:10:17.411401Z","iopub.status.busy":"2024-05-29T08:10:17.410773Z","iopub.status.idle":"2024-05-29T08:10:17.420528Z","shell.execute_reply":"2024-05-29T08:10:17.419434Z","shell.execute_reply.started":"2024-05-29T08:10:17.411365Z"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716911286868,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"aST6B5urlzGG","jupyter":{"outputs_hidden":false},"outputId":"42701bb5-72b4-4a03-d37c-f1ae46c6c7f3","trusted":true},"outputs":[],"source":["# Metodos de la clase : AutoConfig de libreria transformers\n","dir(CONFIG_DEBERTA_V3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T08:14:35.323856Z","iopub.status.busy":"2024-05-29T08:14:35.322674Z","iopub.status.idle":"2024-05-29T08:14:35.331568Z","shell.execute_reply":"2024-05-29T08:14:35.330639Z","shell.execute_reply.started":"2024-05-29T08:14:35.323815Z"},"trusted":true},"outputs":[],"source":["# DEBERTA V3 CONFIG\n","CONFIG_DEBERTA_V3"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d85fd58d-0622-4c21-9032-a5bfb67bff64","_uuid":"3b941c51-d2a4-4883-a6c1-a0e6c30d4dab","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:09:12.453308Z","iopub.status.busy":"2024-05-29T08:09:12.452232Z","iopub.status.idle":"2024-05-29T08:09:12.460207Z","shell.execute_reply":"2024-05-29T08:09:12.459198Z","shell.execute_reply.started":"2024-05-29T08:09:12.453271Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# DEBERTA V2 CONFIG\n","CONFIG_DEBERTA_V2"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3eeafb25-b126-488f-9059-b6496564628a","_uuid":"d8880fe4-d217-4072-acf9-03928cc23f0c","collapsed":false,"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716911286869,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"yv1SBdnVlzGH","jupyter":{"outputs_hidden":false},"outputId":"2b3f7b90-96ec-4015-9444-44872480050b","trusted":true},"outputs":[],"source":["# TOKENIZER CONFIG\n","tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T08:38:45.904729Z","iopub.status.busy":"2024-05-29T08:38:45.904334Z","iopub.status.idle":"2024-05-29T08:38:45.925333Z","shell.execute_reply":"2024-05-29T08:38:45.924272Z","shell.execute_reply.started":"2024-05-29T08:38:45.904700Z"},"trusted":true},"outputs":[],"source":["## Probamos a contar con cuantos registros nos quedariamos si quitasemos de los meses que no sean january y dec porque esos meses no tienen la label 1\n","df_jan_dec = df[(df[\"nombre_mes\"] == 'January') | (df[\"nombre_mes\"] == 'December')]\n","df_jan_dec.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd6c655c-00ca-41ab-a2cf-7d337c7898fd","_uuid":"3e73a0b3-4eae-4db8-955f-4fdf5ec70306","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:39:16.125127Z","iopub.status.busy":"2024-05-29T08:39:16.124779Z","iopub.status.idle":"2024-05-29T08:39:23.809200Z","shell.execute_reply":"2024-05-29T08:39:23.808364Z","shell.execute_reply.started":"2024-05-29T08:39:16.125099Z"},"executionInfo":{"elapsed":8075,"status":"ok","timestamp":1716911294940,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"Y-91wXVQGDNj","jupyter":{"outputs_hidden":false},"outputId":"5a8ed517-6234-4c15-d4c3-70b3ca2bbb0c","trusted":true},"outputs":[],"source":["dataset = TwDataset(\n","                    tokenizer =tokenizer ,\n","                    padding = True,\n","                    truncate = True,\n","                    max_position_embeddings = CONFIG_DEBERTA_V3.max_position_embeddings,\n","                    df =df,\n","                    text_field_name = 'post_text',\n","                    label_field_name = 'label',\n","                    id_field_name = ['post_id']\n","                    )\n","ruta_data_colab = \"/content/drive/MyDrive/NLP/data/data_tw.csv\"\n","ruta_data_kaggle = \"/kaggle/input/data-tw\"\n","_, dataset_split = dataset.get_hg_dataset(split = True, tokenize = True, from_csv = False, csv_path = ruta_data_kaggle )"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"749846c6-84a8-457e-9863-a4120f3a7cd0","_uuid":"5e0c8c8a-8c47-42ab-8082-26a2dc152072","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:39:23.811230Z","iopub.status.busy":"2024-05-29T08:39:23.810936Z","iopub.status.idle":"2024-05-29T08:39:23.826733Z","shell.execute_reply":"2024-05-29T08:39:23.825741Z","shell.execute_reply.started":"2024-05-29T08:39:23.811204Z"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716911294940,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"8-EY4jMgZSQ2","jupyter":{"outputs_hidden":false},"outputId":"ca2868d2-5ab9-41fd-c9b3-a6c2daef3b6a","trusted":true},"outputs":[],"source":["dataset_split[\"train\"][\"labels\"][0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bb07e13-63b5-43bc-b44e-f77aee95db05","_uuid":"f72d9577-622f-41e3-b7e4-f8695955987d","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:39:32.193981Z","iopub.status.busy":"2024-05-29T08:39:32.193610Z","iopub.status.idle":"2024-05-29T08:39:35.580653Z","shell.execute_reply":"2024-05-29T08:39:35.579803Z","shell.execute_reply.started":"2024-05-29T08:39:32.193952Z"},"executionInfo":{"elapsed":3178,"status":"ok","timestamp":1716911298114,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"kTCAktuvevFV","jupyter":{"outputs_hidden":false},"outputId":"4eb99cf4-5349-4b2a-c576-f7d4e4031d14","trusted":true},"outputs":[],"source":["ruta_fig_colab = '/content/drive/MyDrive/NLP/data'\n","ruta_fig_local = \"/content/drive/MyDrive/NLP/data/figures\"\n","dataset.get_plots(dir_path = ruta_fig_colab, figure_name = 'tw_fig', show = True)"]},{"cell_type":"markdown","metadata":{},"source":["Los diagramas anteriores muestran el número de tokens y el número de caracteres por cada texto dentro del corpus e incorporan en tamaño bastante pequeño el ID de dicho tweet. La razón de esto es ver si se está truncando el texto de entrada del modelo DEBERTA porque la secuencia de tokens a lo mejor es muy elevada. Algo que no sucede realmente para ningún texto."]},{"cell_type":"markdown","metadata":{"_cell_guid":"601aafb6-2ee7-47fd-a639-9b29ae9aefc3","_uuid":"00348e29-c183-47aa-8811-0cab56a38e0c","id":"thZnuS2pPacd","trusted":true},"source":["### **CONFIGURACION MODELO DEBERTA**"]},{"cell_type":"markdown","metadata":{},"source":["- paper DEBERTA:\n","\n","He, P., Liu, X., Gao, J., & Chen, W. (2020). DeBERTa: Decoding-enhanced BERT with disentangled attention. *arXiv*. https://doi.org/10.48550/arXiv.2006.03654\n"]},{"cell_type":"markdown","metadata":{"_cell_guid":"62c2ebeb-af1b-4060-a383-9d56bf27aa13","_uuid":"c485111a-fc50-4dfb-a0f0-cbe51bb3a555","id":"Wxz2xGjHV7hK","trusted":true},"source":["#### CONFIGURACION PARA EL MODELO DE CLASIFICACION DE TEXTO [PARA PASARLO A ARCHIVO JSON Y GUARDARLO UNA VEZ SE HA FINE TUNEADO]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f9fd10d-61d7-470c-a389-5ea8f180faa6","_uuid":"2e5daa6c-88c5-40cf-88d5-3d1c2ec9f8d3","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:39:46.668349Z","iopub.status.busy":"2024-05-29T08:39:46.667995Z","iopub.status.idle":"2024-05-29T08:39:46.681383Z","shell.execute_reply":"2024-05-29T08:39:46.680468Z","shell.execute_reply.started":"2024-05-29T08:39:46.668324Z"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1716911771220,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"pd-U0sLEWACX","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["FILE_PATH_CONF = './conf/model.json'\n","CHECK_POINT_PATH = \"./model/checkpoint\"\n","MODEL_SAVE_PATH = \"./model\"\n","\n","# max_position_embeddings : The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).\n","MODEL_CONFG ={\n","  \"model\": {\n","              \"name\": MODEL_NAME,              \n","              \"emergency_save_path\": MODEL_SAVE_PATH+\"/last_params\",\n","              \"config\" : {\n","                            \"_name_or_path\": \"microsoft/deberta-v3-base\",\n","                              \"attention_probs_dropout_prob\": 0.1,\n","                              \"hidden_act\": \"gelu\",\n","                              \"hidden_dropout_prob\": 0.1,\n","                              \"hidden_size\": 768,\n","                              \"initializer_range\": 0.02,\n","                              \"intermediate_size\": 3072,\n","                              \"layer_norm_eps\": 1e-07,\n","                              \"max_position_embeddings\": 512,\n","                              \"max_relative_positions\": -1,\n","                              \"model_type\": \"deberta-v2\",\n","                              \"norm_rel_ebd\": \"layer_norm\",\n","                              \"num_attention_heads\": 12,\n","                              \"num_hidden_layers\": 12,\n","                              \"pad_token_id\": 0,\n","                              \"pooler_dropout\": 0,\n","                              \"pooler_hidden_act\": \"gelu\",\n","                              \"pooler_hidden_size\": 768,\n","                              \"pos_att_type\": [\n","                                \"p2c\",\n","                                \"c2p\"\n","                              ],\n","                              \"position_biased_input\": False,\n","                              \"position_buckets\": 256,\n","                              \"relative_attention\": True,\n","                              \"share_att_key\": True,\n","                              \"transformers_version\": \"4.39.3\",\n","                              \"type_vocab_size\": 0,\n","                              \"vocab_size\": 128100\n","                            }\n","              \n","          },\n","  \"problem\" : {\n","                \"labels_to_pred\" : [0,1]\n","  },\n","  \"trainer\":{\n","              \"callbacks\":{\n","                            \"EarlyStoppingCallback\": {\n","                                                        \"early_stopping_patience\":50,\n","                                                        \"early_stopping_threshold\":0.0\n","                                                      }\n","                          }\n","              },\n","\n","  \"metrics\": {\n","              \"type\":\n","                    {\n","                    \"f1\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\",\"binary\"]},\n","                    \"recall\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\",\"binary\"]},\n","                    \"MulticlassAccuracy\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"None\"]},\n","                    \"precision\":{\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\",\"binary\"]}\n","                    },\n","                \"optimize\": {\"name\":\"precision_binary\"}\n","              },\n","  \"training_args\": {\n","                    \"output_dir\" : CHECK_POINT_PATH,\n","                    \"logging_dir\" : MODEL_SAVE_PATH+\"/logs\",\n","                    \"eval_strategy\" : \"epoch\",\n","                    \"report_to\":\"tensorboard\",\n","                    \"save_strategy\" : \"epoch\",\n","                    \"learning_rate\":3e-04,\n","                    \"per_device_train_batch_size\":36,\n","                    \"per_device_eval_batch_size\":16,\n","                    \"num_train_epochs\":50,\n","                    \"weight_decay\":0.01,\n","                    \"warmup_steps\":500,\n","                    \"adam_epsilon\":1e-08,\n","                    \"load_best_model_at_end\":True,\n","                    \"metric_for_best_model\":\"precision_binary\",\n","                    \"gradient_accumulation_steps\": 10,\n","                    \"warmup_ratio\":0.03,\n","                    \"fp16_full_eval\":False,\n","                    \"fp16\":True,\n","                    \"bf16\":False\n","                  }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f724c968-af51-4a0b-9391-b676679502e4","_uuid":"434d1b96-ed41-47e1-8ad9-6498d8a4740a","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T08:39:51.353187Z","iopub.status.busy":"2024-05-29T08:39:51.352753Z","iopub.status.idle":"2024-05-29T08:39:51.360760Z","shell.execute_reply":"2024-05-29T08:39:51.359654Z","shell.execute_reply.started":"2024-05-29T08:39:51.353152Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["\n","def create_directory(directory_path):\n","    \"\"\"\n","    Check if a directory exists and create it if it doesn't.\n","\n","    Parameters:\n","    directory_path (str): The path of the directory to check and create.\n","    \"\"\"\n","    if not os.path.exists(directory_path):\n","        os.makedirs(directory_path)\n","        print(f\"Directory '{directory_path}' created.\")\n","        return directory_path\n","    else:\n","        print(f\"Directory '{directory_path}' already exists.\")\n","        return directory_path\n","        \n","\n","# Volcamos nueva configuracion del modelo obtenida del Autoconfig en el json[por si cambiasemos de modelo]\n","try:\n","    with open(FILE_PATH_CONF, \"w\") as outfile:\n","        json.dump(MODEL_CONFG, outfile)\n","except Exception as e:\n","    print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"114cfb77-3488-433b-82e8-29b4fe0e7ff0","_uuid":"91a27830-4614-4ffd-86fd-595750ed54ba","collapsed":false,"execution":{"iopub.execute_input":"2024-05-29T09:51:02.338048Z","iopub.status.busy":"2024-05-29T09:51:02.337705Z","iopub.status.idle":"2024-05-29T09:51:02.382097Z","shell.execute_reply":"2024-05-29T09:51:02.381305Z","shell.execute_reply.started":"2024-05-29T09:51:02.338023Z"},"executionInfo":{"elapsed":320,"status":"ok","timestamp":1716912556907,"user":{"displayName":"Jorge Resino","userId":"07175374434332389590"},"user_tz":-120},"id":"mt6t-qgKPc0H","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Modelo clasificacion binaria\n","class BertNet:\n","\n","    def __init__(self, model_conf_path: str, config_mode = 'json', config : dict = None):\n","        self.config_path = model_conf_path\n","        if  config_mode == 'json':\n","            self.config = self._parse_config()\n","        if  config_mode == 'dict':\n","            self.config = config\n","        self.model_name = self._get_model_name()\n","        self.model_tokenizer = self._get_tokenizer()\n","        self.model_config = self._get_model_config()\n","        self.model = self._get_model()\n","        self.compute_metric_f = self._get_metrics()\n","        self.train_args = self._get_training_args()\n","\n","    def _parse_config(self) -> Dict:\n","        if not os.path.exists(self.config_path):\n","            raise FileNotFoundError(f\"Config file not found at {self.config_path}\")\n","        with open(self.config_path, 'r') as file:\n","            config = json.load(file)\n","        return config\n","\n","    def _get_model_name(self):\n","        model = self.config.get(\"model\", {})\n","        model_name = model.get(\"name\", None)\n","        if model_name is None:\n","            raise ValueError(\"Model name not defined in the model config file\")\n","        return model_name\n","\n","    def _get_model_config(self):\n","        problem = self.config.get(\"problem\", {})\n","        self.labels = problem.get(\"labels_to_pred\", None)\n","        if self.labels is None:\n","            raise ValueError(\"Labels to predict not defined in the model config file\")\n","        self.id2label = {i: lab for i, lab in enumerate(self.labels)}\n","        self.label2id = {lab: i for i, lab in enumerate(self.labels)}\n","        self.config[\"model\"][\"config\"][\"id2label\"] = self.id2label\n","        self.config[\"model\"][\"config\"][\"label2id\"] = self.label2id\n","        self.config[\"model\"][\"config\"][\"num_labels\"] = len(self.labels)\n","        try:\n","            return AutoConfig.from_pretrained(\n","                pretrained_model_name_or_path =MODEL_NAME,\n","                config = self.config[\"model\"][\"config\"]\n","            )\n","        except Exception as e:\n","            raise ValueError(f\"Error in get Config Model method: {e}\")\n","\n","    def _get_model(self):\n","        try:\n","            return AutoModelForSequenceClassification.from_pretrained(\n","                                        pretrained_model_name_or_path=self.model_name, \n","                                        config=self.model_config\n","                                        )\n","        except Exception as e:\n","            raise ValueError(f\"Error in get Model method: {e}\")\n","\n","    def _get_tokenizer(self):\n","        try:\n","            max_length = self.config[\"model\"][\"config\"][\"max_position_embeddings\"]\n","        except Exception as e:\n","            print(e)\n","            max_length = 512 # valñr tipico por si no es definido en la configuracion del modelo\n","        try:\n","            return AutoTokenizer.from_pretrained(\n","                                pretrained_model_name_or_path=self.model_name,\n","                                use_fast=True,\n","                                model_max_length =max_length\n","                                )\n","        except Exception as e:\n","            raise ValueError(f\"Error in get tokenizer method: {e}\")\n","\n","    def _get_metrics(self):\n","        metrics = self.config.get(\"metrics\", {})\n","        self.metrics_to_compute = {metric: details[\"method\"] for metric, details in metrics[\"type\"].items() if details[\"compute\"]}\n","        self.opt_metric = metrics.get(\"optimize\", \"\")\n","        return self._get_compute_metric_f(metrics=self.metrics_to_compute)\n","\n","    def _get_compute_metric_f(self, metrics: Dict[str, List[str]]):\n","        master_metric_mapper = {\n","            \"f1_macro\": f1_score,\n","            \"f1_weighted\": f1_score,\n","            \"f1_binary\": f1_score,\n","            \"MulticlassAccuracy_micro\": MulticlassAccuracy(average='micro', num_classes=4, k=1),\n","            \"MulticlassAccuracy_macro\": MulticlassAccuracy(average='macro', num_classes=4, k=1),\n","            \"MulticlassAccuracy_None\": MulticlassAccuracy(average=None, num_classes=4, k=1),\n","            \"recall_micro\": recall_score,\n","            \"recall_macro\": recall_score,\n","            \"recall_weighted\": recall_score,\n","            \"recall_binary\": recall_score,\n","            \"precision_micro\": precision_score,\n","            \"precision_macro\": precision_score,\n","            \"precision_weighted\": precision_score,\n","            \"precision_binary\": precision_score\n","        }\n","        self._metric_obj = {}\n","        for metric, methods in metrics.items():\n","            for method in methods:\n","                key_name = metric + \"_\" + method\n","                metric_obj = master_metric_mapper.get(key_name, None)\n","                if metric_obj is not None:\n","                    self._metric_obj[key_name] = metric_obj\n","\n","        def compute_metrics(pred: EvalPrediction):\n","            \"\"\"Funcion para computo de metricas sobre el conjunto de validacion\n","\n","            Args:\n","                pred (EvalPrediction): ...\n","\n","            Returns:\n","                dict: resultados de las metricas\n","            \"\"\"\n","            predictions, labels = pred\n","            predictions = torch.tensor(predictions)\n","            labels = torch.tensor(labels)\n","\n","            #print(\"Logits shape : \" , predictions.shape)\n","            #print(\"labels shape : \" ,labels.shape)\n","\n","            pred_label = torch.argmax(predictions, dim=1)\n","            # solo descomentar si las target labels se esperan que sean probablidades con shape -> (batch_size,num_clases)\n","            # de lo contarrio labels tipicamente son -> (batch_size) == una true_label por sample == clase de esa sample\n","            # true_label = torch.argmax(labels, dim=1)\n","            #print(\"Prediction label shape: \" , pred_label.shape)\n","            #print(\"true_label shape : \" ,labels.shape)\n","            print(\"Prediction label: \" , pred_label[0:20])\n","            print(\"true_label  : \" ,labels[0:20])\n","\n","            metric_results = {}\n","            for m_name, metric in self._metric_obj.items():\n","                metric_name, method = m_name.rsplit('_', 1)\n","                if metric_name in [\"f1\", \"precision\", \"recall\"]:\n","                    metric_results[m_name] = metric(y_true=labels, y_pred=pred_label, average=method)\n","                elif metric_name == \"MulticlassAccuracy\":\n","                  metric.update(pred_label, labels)\n","                  if m_name == \"MulticlassAccuracy_None\":\n","                    self.multiclass_accuracy_none.append(metric.compute()) # porque devuelve yensor que no es serializable y da error metodo train\n","                  else:\n","                    metric_results[m_name] = metric.compute()\n","\n","            cross_entropy_loss_f = nn.CrossEntropyLoss()\n","            loss = cross_entropy_loss_f(predictions, labels)\n","            metric_results[\"Train Cross entropy loss\"] = loss.item()\n","            metric_results[\"Inverse Train Cross entropy loss\"] = 1 / loss.item()\n","\n","            return metric_results\n","        return compute_metrics\n","\n","    def _get_training_args(self):\n","        training_args = self.config.get(\"training_args\", {})\n","        return TrainingArguments(\n","            output_dir=training_args.get(\"output_dir\", \"./model/DEFAULT/checkpoint\"),\n","            logging_dir = training_args.get(\"logging_dir\", \"./model/DEFAULT/logs\"),\n","            evaluation_strategy=training_args.get(\"evaluation_strategy\", \"epoch\"),\n","            report_to=training_args.get(\"report_to\", \"tensorboard\"),\n","            save_strategy=training_args.get(\"save_strategy\", \"epoch\"),\n","            learning_rate=training_args.get(\"learning_rate\", 3e-05),\n","            per_device_train_batch_size=training_args.get(\"per_device_train_batch_size\", 20),\n","            per_device_eval_batch_size=training_args.get(\"per_device_eval_batch_size\", 32),\n","            num_train_epochs=training_args.get(\"num_train_epochs\", 3),  # Ajustado para tiempos de prueba\n","            weight_decay=training_args.get(\"weight_decay\", 0.01),\n","            warmup_steps=training_args.get(\"weight_decay\", 0.01),\n","            adam_epsilon=training_args.get(\"warmup_steps\", 100),\n","            load_best_model_at_end=training_args.get(\"load_best_model_at_end\", True),\n","            metric_for_best_model=training_args.get(\"metric_for_best_model\", self.opt_metric),\n","            gradient_accumulation_steps=training_args.get(\"gradient_accumulation_steps\", 10),\n","            warmup_ratio=training_args.get(\"warmup_ratio\", 0.03),\n","            fp16_full_eval=training_args.get(\"fp16_full_eval\", False),\n","            fp16=training_args.get(\"fp16\", True),\n","            bf16=training_args.get(\"bf16\", False)\n","        )\n","    def _get_data_collator(self):\n","        return DataCollatorWithPadding(tokenizer=self.model_tokenizer)\n","\n","    def _get_trainer(self, dataset: DatasetDict):\n","        trainer_config = self.config.get(\"trainer\", {})\n","        callbacks_args = trainer_config.get(\"callbacks\", {})\n","        early_stopping_args = callbacks_args.get(\"EarlyStoppingCallback\", {})\n","        self.data_collator = self._get_data_collator()\n","        return Trainer(\n","            model=self.model,\n","            args=self.train_args,\n","            train_dataset=dataset[\"train\"],\n","            eval_dataset=dataset[\"validation\"],\n","            tokenizer=self.model_tokenizer,\n","            compute_metrics=self.compute_metric_f,\n","            #data_collator =  self.data_collator,\n","            callbacks=[\n","                EarlyStoppingCallback(\n","                    early_stopping_patience=early_stopping_args.get(\"early_stopping_patience\", 3),\n","                    early_stopping_threshold=early_stopping_args.get(\"early_stopping_threshold\", 0.0)\n","                ),\n","                TensorBoardCallback()\n","            ]\n","        )\n","    \n","\n","    def train(self, dataset: DatasetDict, emergency_save:bool = False, resume_from_checkpoint:Union[bool,str] =False):\n","        \"\"\"\n","        Paarameters:\n","            resume_from_checkpoint : bool -> True busca output_dir ultimo check point / ruta str -> del check point a usar\n","        \"\"\"\n","        self.multiclass_accuracy_none = [] # porque devuelve tensor que no es serializable y da error metodo train\n","        self.trainer = self._get_trainer(dataset)\n","        try:\n","            self.trainer.train(resume_from_checkpoint = resume_from_checkpoint)\n","        except Exception as e:\n","            print(f\"Error durante training : {e}\")\n","        finally:\n","            if emergency_save:\n","                try:\n","                    self.save(path=self.config[\"model\"].get(\"emergency_save_path\", \"./model/last_params\"))\n","                except Exception as e:\n","                    print(e)\n","                    \n","    def predict(self, dataset: DatasetDict):\n","        self.trainer = self._get_trainer(dataset)\n","        try:\n","            self.trainer.evaluate(dataset)\n","        except Exception as e:\n","            print(f\"Error durante prdicion : {e}\")\n","\n","    def save(self, path:str):\n","        \"\"\" Metodo que guarda parametros del modelo y su optimizer\"\"\"\n","        path = BertNet.create_directory(path=path)\n","        model_save_path = os.path.join(path, 'model.pt')\n","        optimizer_save_path = os.path.join(path, 'optimizer.pt')\n","        torch.save(self.trainer.model.state_dict(), model_save_path)\n","        torch.save(self.trainer.optimizer.state_dict(), optimizer_save_path)\n","                          \n","    @staticmethod\n","    def create_directory(path:str):\n","        \"\"\"\n","        Check if a directory exists and create it if it doesn't.\n","\n","        Parameters:\n","        path (str): The path of the directory to check and create.\n","        \"\"\"\n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","            print(f\"Directory '{path}' created.\")\n","            return path\n","        else:\n","            print(f\"Directory '{path}' already exists.\")\n","            return path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Funcion para el borrado de checkpoints en kaggle notebooks:\n","def remove_folder_contents(folder):\n","    for the_file in os.listdir(folder):\n","        file_path = os.path.join(folder, the_file)\n","        try:\n","            if os.path.isfile(file_path):\n","                os.unlink(file_path)\n","            elif os.path.isdir(file_path):\n","                remove_folder_contents(file_path)\n","                os.rmdir(file_path)\n","        except Exception as e:\n","            print(e)\n","\n","folder_path = '/kaggle/working/model/checkpoint/checkpoint-289'\n","remove_folder_contents(folder_path)\n","os.rmdir(folder_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()\n","device = torch.device(\"cuda\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instancia el modelo segun su dict/json de configuracion\n","tw_bert = BertNet(\n","                model_conf_path = FILE_PATH_CONF, \n","                config_mode = 'dict',\n","                config = MODEL_CONFG\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train partiendo o no de un checkpoint\n","CHECK_POINT_PATH = \"/kaggle/working/model/checkpoint/checkpoint-338\"\n","# CHECK_POINT_PATH = True # carga el ultimo checkpoint en output_dir \n","\n","tw_bert.train(\n","                dataset=dataset_split, \n","                emergency_save = True, \n","                resume_from_checkpoint = False\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predictions results\n","results = tw_bert.predict(dataset_split[\"test\"])\n","results"]},{"cell_type":"markdown","metadata":{},"source":["**Como se observa en los errores, es complicado hacer fine-tuning de este tipo de modelos si no se tiene una licencia premium de Colab/Kaggle o una GPU de última generación. No es mi caso, por lo que en local me es imposible hacer el fine-tuning de un modelo tan grande con tanta cantidad de capas de encoder, formadas a su vez por numerosas multi-head attention y además de las FFN. Todo esto hace complicado utilizar este tipo de modelos en local a no ser que sean cuantizados. Por esta razón, aunque en Kaggle conseguí entrenar un poco el modelo, no lo suficiente para obtener buenos resultados, tuve que buscar alternativas. A continuación, se muestra la solución que me propuse a este problema que me surgió.**"]},{"cell_type":"markdown","metadata":{},"source":["## **INTENTO PYTORCH NATIVO HACIENDO USO DE FEED FORWARD NETWORK + CONVOLUTIONAL NETWORK PARA LA CLASIFICACIÓN BINARIA DE TEXTOS**"]},{"cell_type":"markdown","metadata":{},"source":["SE HACE USO DE LAS CLASES DATASET Y DATALOADER DE  TORCH NATIVO PARA LA CREACION DEL DATASET DE TRAIN, TEXT Y VALIDACION"]},{"cell_type":"markdown","metadata":{},"source":["El enfoque que se va a dar en este caso es generar embeddings contextualizados usando un modelo SBERT (siamese BERT) cuyo paper se expone más abajo. Se trata de un modelo \"Sentence Transformer\" que asigna frases y párrafos a un espacio vectorial denso de 384 dimensiones y puede utilizarse para tareas como la agrupación o la búsqueda semántica.\n","\n","Con este modelo se generan estos vectores o embeddings de alta dimensionalidad para el campo [\"post_text\"] del dataset y después se hace uso de una red FFN+CV para la clasificación binaria de estos textos, haciendo uso de varias métricas sobre el subconjunto de validación y varios criterios de parada. Se aplica este enfoque tras aplicar varias técnicas de preprocesado diferentes y se exponen los resultados para su comparación.\n","\n","También destaca que no se hace uso de features originalmente existentes (aparte de \"post_text\") como: 'post_id', 'post_created', 'user_id', 'followers', 'friends', 'favourites', 'statuses', 'retweets', 'día', 'mes', 'año', 'h', 'min', 'seg', 'día_semana', 'nombre_mes'. Esta decisión se basa en el hecho de intentar predecir la etiqueta binaria solo usando los textos y sus embeddings como reto personal. Por otro lado, para mejorar el modelo se podría aplicar algún tipo de clustering previo utilizando estas features o metadatos de cada texto no utilizados."]},{"cell_type":"markdown","metadata":{},"source":["- Model card:\n","\n","SentenceTransformer(\n"," \n","  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n","  \n","  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",")\n","\n","- Paper del modelo Siamese BERT:\n","Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. arXiv.\n"," https://doi.org/10.48550/arXiv.1908.10084\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","\n","class TwTorchData(Dataset):\n","    def __init__(self, df: pd.DataFrame, label_field_name : str, text_field_name : str) :\n","        super().__init__()\n","        self.data = df\n","        self.label_field_name  = label_field_name\n","        self.text_field_name  = text_field_name\n","        #\n","        self.embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n","        # Create samples and target codify labels to train net\n","        self._map_labels()\n","        self.x = torch.tensor(self._get_embeddings(self.data.loc[:,self.text_field_name].to_list()))\n","        self.y = torch.tensor(self.data.loc[:,'maped_labels'].values)\n","\n","    def __getitem__(self, index):\n","        return self.x[index] ,self.y[index]\n","    def __len__(self):\n","        return self.data.shape[0]\n","    def __repr__(self) -> str:\n","        return f\"x shape :{self.x.shape} //  y shape : {self.y.shape}\"\n","\n","    def _map_labels(self):\n","        mapping = {}\n","        for i,l_i in enumerate(self.data[self.label_field_name].unique()):\n","            mapping[l_i] = i \n","        self.data['maped_labels'] = self.data[self.label_field_name].map(mapping)\n","         \n","    def _get_embeddings(self, texts: List[str]) -> List[float]:\n","        return self.embedding_model.embed_documents(texts)\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.loc[:,\"post_text\"].to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"label_name\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"label\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Twitter dataset\n","torch.cuda.empty_cache()\n","# CPU mode\n","device = torch.device(\"cuda\")\n","device"]},{"cell_type":"markdown","metadata":{},"source":["**Creación de datasets con difrentes preprocesamientos para entrenar el mismo modelo, predecir y evaluar dichos preprocesaminetos en función de la performance del modelo**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_solo_jan_dec.reset_index(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check len de los df y el corpus de los TextProcessors\n","print(len(preprocessor_bow.corpus))\n","print(len(preprocessor_bow_tf_idf.corpus))\n","print(len(preprocessor_bow_tf_idf_jan_dec.corpus))\n","print((df.shape))\n","print((df_solo_jan_dec.shape))"]},{"cell_type":"markdown","metadata":{},"source":["**Comprobamos que han sido preprocesados con una pequeña sample**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessor_bow.corpus[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.loc[0:10,]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessor_bow_tf_idf_jan_dec.corpus[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_solo_jan_dec.loc[0:10,\"post_text\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creacion de nuevos dataframes a partir de los corpus preprocesados\n","df_clean_1 = df.copy()\n","df_clean_2 = df.copy()\n","df_clean_jd = df_solo_jan_dec.copy()\n","for i in range(0,df.shape[0]):\n","    df_clean_1.loc[i,\"post_text\"] = preprocessor_bow.corpus[i]\n","    df_clean_2.loc[i,\"post_text\"] = preprocessor_bow_tf_idf.corpus[i]\n","for i in range(0,df_solo_jan_dec.shape[0]):\n","    df_clean_jd.loc[i,\"post_text\"] = preprocessor_bow_tf_idf_jan_dec.corpus[i]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Twitter datasets\n","TW_TORCH_DATA_CLEAN_1 = TwTorchData(df = df_clean_1, label_field_name ='label', text_field_name = 'post_text') \n","TW_TORCH_DATA_CLEAN_2 = TwTorchData(df = df_clean_2, label_field_name ='label', text_field_name = 'post_text')\n","TW_TORCH_DATA_JD = TwTorchData(df = df_clean_jd, label_field_name ='label', text_field_name = 'post_text')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(TW_TORCH_DATA_CLEAN_1)\n","print(TW_TORCH_DATA_CLEAN_2)\n","print(TW_TORCH_DATA_JD)"]},{"cell_type":"markdown","metadata":{},"source":["### Papers de las técnicas utilizadas y en las que me he basado para crear esta red de clasificacion de texto:\n","\n","- Kim, Y. (2014). Convolutional neural networks for sentence classification. *arXiv*. https://doi.org/10.48550/arXiv.1408.5882\n","\n","- Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. *arXiv* https://arxiv.org/abs/1502.03167\n","\n","- Liu, Z., Xu, Z., Jin, J., Shen, Z., & Darrell, T. (2023). Dropout reduces underfitting. *arXiv*. https://doi.org/10.48550/arXiv.2303.01500\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NeuralConvNet(nn.Module):\n","    def __init__(self,num_classes, input_size : int, p :float = 0.3, batch_norm : bool = True) -> None:\n","        super().__init__()\n","        \n","        self.drop_out_p = p\n","        self.batch_norm = batch_norm\n","        # (384) -> width and heigth = np.floor(np.sqrt(input_size) )\n","        self.input_size = input_size\n","        self.width , self.heigth =  self._factorize_input_size()\n","        \n","        # (BATCH,1,self.width, self.heigth) -> si input_size = 384 -> (BATCH,1,self.width , self.heigth)\n","        # 1º conv \n","        p1,s1,k1,outc1 = 0,1,5,6\n","        self.conv_1 = nn.Conv2d(in_channels = 1 , out_channels =outc1, kernel_size =k1 , stride=s1, padding=p1)\n","        w2,h2 = self._get_sizes_conv(padding=p1,stride=s1, kernel=k1,  w=self.width,h =self.heigth)\n","        self.relu_1 = nn.ReLU()\n","        self.batch_norm_1 = nn.BatchNorm2d(num_features = outc1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.drop_1 = nn.Dropout2d(p=self.drop_out_p)\n","        \n","        print(f\"After 1st conv layer: {w2} x {h2}\") # Expected: (BATCH, 6, 12, 20)\n","        \n","        # 1º MAX POOL\n","        pm1,sm1,km1 = 0,2,2\n","        self.maxp_1 = nn.MaxPool2d(kernel_size = km1 , stride=sm1, padding=pm1)\n","        w2,h2 =  self._get_sizes_maxp(padding=pm1,stride=sm1, kernel=km1,  w=w2,h =h2)\n","        print(f\"After 1st max pool layer: {w2} x {h2}\") # Expected: (BATCH, 6, 6, 10)\n","\n","        # 2º conv \n","        p2,s2,k2,outc2 = 0,1,5,16\n","        self.conv_2 = nn.Conv2d(in_channels =outc1,out_channels=outc2,kernel_size =k2,stride=s2,padding=p2)\n","        w3,h3 = self._get_sizes_conv(padding=p2,stride=s2, kernel=k2,  w=w2,h =h2)\n","        self.relu_2 = nn.ReLU()\n","        self.drop_2 = nn.Dropout2d(p=self.drop_out_p)\n","        self.batch_norm_2 = nn.BatchNorm2d(num_features = outc2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        print(f\"After 2nd conv layer: {w3} x {h3}\") # Expected: (BATCH, 16, 2, 6)\n","\n","        # 2º MAX POOL\n","        pm2,sm2,km2 = 0,2,2\n","        self.maxp_2 = nn.MaxPool2d(kernel_size = km2 , stride=sm2, padding=pm2)\n","        w3,h3 =  self._get_sizes_maxp(padding=pm2,stride=sm2, kernel=km2,  w=w3,h =h3)\n","        print(f\"After 2nd max pool layer: {w3} x {h3}\") # Expected: (BATCH, 16, 1, 3)\n","        \n","        # LINEAR LAYER\n","        self.final_size = outc2*w3*h3\n","        self.l_1 = nn.Linear(in_features = self.final_size , out_features = num_classes, bias=True)\n","        \n","    def forward(self,x):\n","        x = x.view(-1,1,self.width , self.heigth) #  (BATCH,384) -> (BATCH,1,16,24) -> (BATCH,1,384)\n","        out = self.conv_1(x)\n","        out = self.relu_1(out)\n","        out = self.drop_1(out)\n","        if self.batch_norm:\n","            out = self.batch_norm_1(out)\n","        out = self.maxp_1(out)\n","        out = self.conv_2(out)\n","        out = self.relu_2(out)\n","        out = self.drop_2(out)\n","        if self.batch_norm:\n","            out = self.batch_norm_2(out)\n","        out = self.maxp_2(out)\n","        out = out.view(out.shape[0], -1) # Flatten\n","        return self.l_1(out)\n","\n","    \n","    def _factorize_input_size(self):\n","        \"\"\" \n","        factoriza size entrada en dos numeros para crear input matrix\n","\n","        Parameters:\n","        input_height (int): Height of the input volume.\n","        input_width (int): Width of the input volume.\n","        kernel_size (int): Size of the pooling kernel (assuming square kernel).\n","        stride (int): Stride of the pooling operation.\n","        padding (int): Padding added to the input volume.\n","\n","        Returns:\n","        tuple: (output_width , output_height)\n","        \"\"\"\n","        for i in range(int(np.sqrt(self.input_size)), 0, -1):\n","            if self.input_size % i == 0:\n","                return i, self.input_size // i\n","                \n","    def _get_sizes_conv(self, padding : int,stride :int, kernel : int,  w : int , h:int):\n","        \"\"\"\n","        Calculate the output dimensions of a conv 2d layer.\n","\n","        Parameters:\n","        h (int): Height of the input volume.\n","        w (int): Width of the input volume.\n","        kernel_size (int): Size of the pooling kernel (assuming square kernel).\n","        stride (int): Stride of the pooling operation.\n","        padding (int): Padding added to the input volume.\n","\n","        Returns:\n","        tuple: (output_width , output_height)\n","        \"\"\"\n","        if stride == 0:\n","            raise ValueError(\"Stride must be not 0\")\n","        return int((((w-kernel) + (2*padding)) / (stride)) + 1), int((((h-kernel) + (2*padding)) / (stride)) + 1)\n","\n","    def _get_sizes_maxp(self, padding : int,stride :int, kernel : int,  w : int , h:int):\n","        \"\"\"\n","        Calculate the output dimensions of a max pooling layer.\n","\n","        Parameters:\n","        h (int): Height of the input volume.\n","        w (int): Width of the input volume.\n","        kernel (int): Size of the pooling kernel (assuming square kernel).\n","        stride (int): Stride of the pooling operation.\n","        padding (int): Padding added to the input volume.\n","\n","        Returns:\n","        tuple: (output_height, output_width)\n","        \"\"\"\n","        return int(np.floor((w - kernel + 2 * padding) / stride) + 1), int(np.floor((h - kernel + 2 * padding) / stride) + 1)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Trainer class (simula Trainer de la liberria transformers de Huggingface pero permite un mayor control de la funcion de perdida que se quiera utilizar, del optmizador y sus hiperparametros, las meticas de evalucion sobre el conjunto de validacion aplicadas mediante la funcion eval_metric, el plotting de las metricas y perdida etc)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class Trainer:\n","    def __init__(self, model, dataset, loss, optimizer, eval_metric : Callable,  configuration :Union[dict,str], plot : Optional[bool] = False):\n","        \"\"\"_summary_\n","\n","        Args:\n","            model (_type_): _description_\n","            optimizer (_type_): _description_\n","            eval_metric (Callable): _description_\n","            configuration (Union[dict,str]): _description_\n","            plot (Optional[bool], optional): _description_. Defaults to False.\n","        \"\"\"\n","        self.config = self.get_config(configuration)\n","        self.dataset = dataset\n","        self.model = model\n","        self.opt = optimizer\n","        self.loss = loss\n","        self.eval_metric = eval_metric\n","        self.plot = plot\n","        self.batch_size = self.config[\"training_args\"].get(\"batch_size\", 100)\n","        self.train_loader = torch.utils.data.DataLoader(dataset = dataset, batch_size = self.batch_size, shuffle = True)\n","        self.test_loader = torch.utils.data.DataLoader(dataset = dataset, batch_size = self.batch_size, shuffle = False)\n","        self.metrics_df = pd.DataFrame()\n","\n","\n","    def get_config(self, configuration) ->dict:\n","        if  isinstance(configuration , dict):\n","            return configuration\n","        if  isinstance(configuration , str):\n","            if not os.path.exists(configuration):\n","                raise FileNotFoundError(f\"Config file not found at {configuration}\")\n","            with open(configuration, 'r') as file:\n","                config = json.load(file)\n","            return config\n","        \n","    def train(self):\n","        BATCH_SIZE = self.batch_size\n","        EPOCHS = self.config[\"training_args\"].get(\"num_train_epochs\", 50)\n","        \n","        val_samples = int(BATCH_SIZE * 0.3)  # 30% for validation \n","        train_samples = int(BATCH_SIZE - val_samples)  # 70% for training\n","\n","        # Setup matplotlib figure outside the loop\n","        if self.plot:\n","            plt.figure(figsize=(12, 8))\n","            ax = plt.gca()  # Get the current axis\n","\n","        #initializing progress bar objects\n","        epoch_loop=tqdm(range(EPOCHS), desc =\"Epoch loop\" , leave=True, position = 0)\n","        \n","        for e in epoch_loop:\n","            \n","            val_loss_ = []\n","            train_loss_ = []\n","            iter_ = []\n","            \n","            # Val set predictions and val set labels\n","            self.val_labels = []\n","            self.val_preds = []\n","            \n","            for i, (samples, labels) in enumerate(self.train_loader):\n","                if samples.shape[0] != BATCH_SIZE:\n","                    print(\"samples.shape[0] != BATCH_SIZE\")\n","                    continue\n","                \n","                train_batch = samples[:train_samples, :].to(device)\n","                train_labels = labels[:train_samples].to(device)\n","                val_batch = samples[train_samples:, :].to(device)\n","                val_labels = labels[train_samples:].to(device)\n","\n","                # Forward pass on the training set\n","                pred_labels = self.model(train_batch)\n","\n","                # Forward pass on the validation set\n","                with torch.no_grad():\n","                    pred_val_labels = self.model(val_batch)\n","                    \n","                    # Eval metric iteration criteria update\n","                    if self.config[\"training_args\"].get(\"eval_strategy\", None) ==\"iter\":\n","                        self.update_eval_metrics(metrics =self.eval_metric(pred=pred_val_labels.cpu(),target=val_labels.cpu())) \n","                    \n","                    # acumulated val set labels and val set pred batch to eval for each epoch if criteria is epoch\n","                    if  self.config[\"training_args\"].get(\"eval_strategy\", None) ==\"epoch\":\n","                        self.val_labels.append(val_labels)\n","                        self.val_preds.append(pred_val_labels)\n","                    \n","                # Compute training loss\n","                train_loss = self.loss(pred_labels, train_labels)\n","                \n","                # Compute validation loss\n","                val_loss = self.loss(pred_val_labels, val_labels)\n","                \n","                    \n","                iter_.append(i)\n","                train_loss_.append(train_loss.item())\n","                val_loss_.append(val_loss.item())\n","\n","                # Backward pass and optimize\n","                train_loss.backward()\n","                self.opt.step()\n","                self.opt.zero_grad()\n","                    \n","                    \n","            # After all batches, compute epoch metrics\n","            if self.config[\"training_args\"].get(\"eval_strategy\", None) ==\"epoch\":\n","                self.update_eval_metrics(metrics =self.eval_metric(\n","                                                        pred=torch.cat( self.val_preds, 0).cpu() ,\n","                                                        target=torch.cat( self.val_labels, 0).cpu(),\n","                                                        extra_metrics={\n","                                                                        \"Val_mean_loss\":np.mean(val_loss_),\n","                                                                        \"Train_mean_loss\":np.mean(train_loss_)\n","                                                                        }\n","                                                                    )\n","                                         )\n","                        \n","            # print mean loss trhough all the batch train and val acumulate losses\n","            epoch_loop.set_description(f\"Val epoch mean loss: {np.mean(val_loss_):.3f}  -- Train epoch mean loss: {np.mean(train_loss_):.3f}\" )\n","            epoch_loop.update()\n","\n","            # Update validation curves\n","            if self.plot:\n","                try:\n","                    if len(train_loss_) > 0 and len(val_loss_) > 0:\n","                        ax.plot(np.arange(e * len(self.train_loader), len(self.train_loader) * (e + 1)), train_loss_, 'red', alpha=0.5, label=f'Train mean loss: {np.mean(train_loss_):.4f} - epoch {e+1}')\n","                        ax.plot(np.arange(e * len(self.train_loader), len(self.train_loader) * (e + 1)), val_loss_, 'green', alpha=0.8, label=f'Val mean loss: {np.mean(val_loss_):.4f} - epoch {e+1}')\n","                        ax.legend()\n","                        ax.set_xlabel('Iteration')\n","                        ax.set_ylabel('Loss')\n","                        ax.set_title(f'Training and Validation Loss -- epoch {e+1}')\n","                        ax.grid(True)\n","                        clear_output(wait=True)\n","                        display(ax.figure)  # Display the figure being updated\n","                        plt.close()  # Close the plot when the training is done\n","                except Exception as e:\n","                    pass\n","\n","                    \n","    def update_eval_metrics(self, metrics :dict):\n","        # Convert the input metrics dict to a DataFrame\n","        metrics_df = pd.DataFrame([metrics])\n","        # Append the new metrics to the existing DataFrame\n","        self.metrics_df = pd.concat([self.metrics_df, metrics_df], ignore_index=True)\n","        # Display the updated metrics table\n","        clear_output(wait=False)\n","        display(self.metrics_df)\n","\n","\n","            \n","    def predict(self):\n","        \n","        # Evaluation / metrics \n","        with torch.no_grad():\n","            accuracy = 0\n","            accuracy_2 = 0  \n","            samples = 0\n","            for i, (b_samples, b_labels) in enumerate(self.test_loader):\n","                b_samples = b_samples.to(device)\n","                b_labels = b_labels.to(device)\n","                \n","                # Número de muestras en el batch\n","                batch_size = b_samples.size(0)\n","                samples += batch_size\n","                \n","                # Predicción\n","                y_test_pred = self.model(b_samples)\n","                \n","                # Método de accuracy 1\n","                pred_class = y_test_pred.argmax(dim=1)\n","                accuracy += (pred_class == b_labels).sum().item()\n","                #print(f\"Accuracy 1 for iter/batch {i}: {(pred_class == b_labels).sum().item()}\")\n","                \n","                # Método de accuracy 2\n","                _, pred_class_2 = torch.max(y_test_pred, dim=1)\n","                accuracy_2 += (pred_class_2 == b_labels).sum().item()\n","                #print(f\"Accuracy 2 for iter/batch {i}: {(pred_class_2 == b_labels).sum().item()}\")\n","\n","            accuracy = accuracy / samples\n","            accuracy_2 = accuracy_2 / samples \n","            print(\"Total accuracy 1:\", accuracy)\n","            print(\"Total accuracy 2:\", accuracy_2)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Función que se aplica sobre el conjunto de validación en cada época o en cada iteración según se quiera. Sirve como criterio de parada del entrenamiento del modelo si la métrica deseada y programada está cierto número de iteraciones sin mejorar.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_metrics(pred, target, extra_metrics :Optional[Dict[str,float]] = None ) -> Dict[str,float]:\n","  metric_results = {}\n","  metrics ={ \"f1\" : f1_score, \"precision\" : precision_score , \"recall\" : recall_score}\n","  try:\n","    pred_label = torch.argmax(pred, dim=1)\n","    for metric_name, metric in metrics.items():\n","      metric_results[metric_name+\"_macro\"] = metric(y_true=target, y_pred=pred_label, average=\"macro\")\n","      metric_results[metric_name+\"_micro\"] = metric(y_true=target, y_pred=pred_label, average=\"micro\")\n","      metric_results[metric_name+\"_weighted\"] = metric(y_true=target, y_pred=pred_label, average=\"weighted\")\n","      metric_results[metric_name+\"_binary\"] = metric(y_true=target, y_pred=pred_label, average=\"binary\")\n","    if extra_metrics is not None:\n","      for extra_name,val in extra_metrics.items():\n","        metric_results[extra_name] = val\n","  except Exception as e:\n","    print(f\"Error eval function : {e}\\nINFO:\\n\")\n","    print(\"Logits shape : \" , pred.shape)\n","    print(\"labels shape : \" ,target.shape)\n","    print(\"pred_label shape : \" , pred_label.shape)\n","    print(\"Prediction label: \" , pred_label[0:5])\n","    print(\"true_label  : \" ,target[0:5])\n","  finally:\n","    return metric_results"]},{"cell_type":"markdown","metadata":{},"source":["Limpieza de la memoria cache del device [cuda, cpu]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()\n","device = torch.device(\"cuda\")\n","device"]},{"cell_type":"markdown","metadata":{},"source":["**MODEL CONFIG donde indican sus hiperparametros y para llevar un track del modelo en sí**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FILE_PATH_CONF = './conf/model_CONV_NET.json'\n","CHECK_POINT_PATH = \"./model/checkpoint_conv_net\"\n","MODEL_SAVE_PATH = \"./model\"\n","\n","CONFIG ={\n","            \"model\": {\n","                        \"name\": \"CONV_NET\",              \n","                        \"emergency_save_path\": MODEL_SAVE_PATH+\"/conv_net\",\n","                        \"config\" : {\n","                                    \"drop_out_prob\" : 0.0,\n","                                    \"batch_norm\" : True\n","                                  }\n","                        \n","                    },\n","            \"problem\" : {\n","                          \"labels\" : df['label'].unique().tolist(),\n","                          \"num_labels\" : df['label'].nunique(),\n","                          },\n","            \"metrics\": {\n","                        \"type\":\n","                              {\n","                              \"f1\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\",\"binary\"]},\n","                              \"recall\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\",\"binary\"]},\n","                              \"MulticlassAccuracy\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"None\"]},\n","                              \"precision\":{\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\",\"binary\"]}\n","                              },\n","                          \"optimize\": {\"name\":\"precision_binary\"}\n","                        },\n","            \"training_args\": {\n","                              \"output_dir\" : CHECK_POINT_PATH,\n","                              \"eval_strategy\" : \"epoch\", #or iter\n","                              \"save_strategy\" : \"epoch\",\n","                              \"load_best_model_at_end\":True,\n","                              \"report_to\":\"\",\n","                              \"batch_size\":100,\n","                              \"num_train_epochs\":100,\n","                              \"optimizer\":{\n","                                              \"learning_rate\":1e-03,\n","                                              \"adam_epsilon\":1e-08,\n","                                              \"beta_1\": 0.9,\n","                                              \"beta_2\": 0.999,\n","                                              \"weight_decay\":0.01,\n","                                              \"warmup_steps\":500,\n","                                              \"gradient_accumulation_steps\": 10,\n","                                              \"warmup_ratio\":0.03,\n","                                            },\n","                              \"fp16_full_eval\":\"\",\n","                              \"fp16\":\"\",\n","                              \"bf16\":\"\"\n","                            }\n","          }\n","\n","\n","\n","def create_directory(directory_path):\n","    \"\"\"\n","    Check if a directory exists and create it if it doesn't.\n","\n","    Parameters:\n","    directory_path (str): The path of the directory to check and create.\n","    \"\"\"\n","    if not os.path.exists(directory_path):\n","        os.makedirs(directory_path)\n","        print(f\"Directory '{directory_path}' created.\")\n","        return directory_path\n","    else:\n","        print(f\"Directory '{directory_path}' already exists.\")\n","        return directory_path\n","        \n","\n","# Volcamos nueva configuracion del modelo obtenida del Autoconfig en el json[por si camboiasemos de modelo]\n","try:\n","    with open(FILE_PATH_CONF, \"w\") as outfile:\n","        json.dump(CONFIG, outfile)\n","except Exception as e:\n","    print(e)"]},{"cell_type":"markdown","metadata":{},"source":["**Definición del modelo, el optimizador y la función de perdida en función de la configuración elegida**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TW_TORCH_DATA_CLEAN_1.x.shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MODEL\n","model = NeuralConvNet(  \n","                    num_classes = CONFIG[\"problem\"].get(\"num_labels\"), \n","                    input_size = TW_TORCH_DATA_CLEAN_1.x.shape[1], # 384 -> SBERT embeddings dimension\n","                    p = CONFIG[\"model\"][\"config\"].get(\"drop_out_prob\",0.2),\n","                    batch_norm =  CONFIG[\"model\"][\"config\"].get(\"batch_norm\", False)\n","                    ).to(device)\n","# OPTIMIZER - ADAM\n","optimizer = torch.optim.Adam(\n","                            params = model.parameters(), \n","                            lr=CONFIG[\"training_args\"][\"optimizer\"].get(\"learning_rate\",0.2), \n","                            betas=(\n","                                    CONFIG[\"training_args\"][\"optimizer\"].get(\"beta_1\",0.9),\n","                                    CONFIG[\"training_args\"][\"optimizer\"].get(\"beta_2\",0.999)\n","                                    ), \n","                            eps=CONFIG[\"training_args\"][\"optimizer\"].get(\"adam_epsilon\",1e-08))\n","\n","# LOSS - CROSS ENTROPY LOSS\n","#loss = nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n","loss = nn.CrossEntropyLoss()\n"]},{"cell_type":"markdown","metadata":{},"source":["### **Definición de los trainers**"]},{"cell_type":"markdown","metadata":{},"source":["Se definen varios trainers. En este caso cada uno comparte modelo, loss y optimizer pero diferente dataset "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TRAINERS\n","trainer_df_1 = Trainer(\n","                model = model , \n","                dataset=TW_TORCH_DATA_CLEAN_1 ,\n","                loss=loss,\n","                optimizer=optimizer,\n","                eval_metric=compute_metrics,  \n","                configuration=CONFIG, \n","                plot = False)\n","\n","\n","trainer_df_2 = Trainer(\n","                model = model , \n","                dataset=TW_TORCH_DATA_CLEAN_2 ,\n","                loss=loss,\n","                optimizer=optimizer,\n","                eval_metric=compute_metrics,  \n","                configuration=CONFIG, \n","                plot = False)\n","\n","trainer_df_jd = Trainer(\n","                model = model , \n","                dataset=TW_TORCH_DATA_JD ,\n","                loss=loss,\n","                optimizer=optimizer,\n","                eval_metric=compute_metrics,  \n","                configuration=CONFIG, \n","                plot = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer_df_1.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer_df_2.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer_df_jd.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer_df_1.predict()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer_df_2.predict()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer_df_jd.predict()"]},{"cell_type":"markdown","metadata":{},"source":["# **Resultados obtenidos y conclusión:**"]},{"cell_type":"markdown","metadata":{},"source":["Para evaluar la predicción del modelo sobre el conjunto de test, se ha utilizado la métrica de precisión o accuracy. Como evaluación del entrenamiento del modelo en cada época (es decir, después de cada feed-forward, cálculo de pérdida, retropropagación y paso del optimizador para cada uno de los lotes) se han aplicado varias métricas y dentro de estas, varios métodos.\n","\n","En concreto:\n","\n","- F1 score\n","- Recall\n","- Precision\n","\n","Y como métodos:\n","\n","- Micro\n","- Macro\n","- Weighted\n","- Binary\n","\n","Como observaciones destacables están el sobreajuste de los dos primeros modelos (entrenados con los dos primeros dataframes) debido a que su accuracy posterior sobre el conjunto de test es bastante más baja que sobre el conjunto de entrenamiento. Es decir, sufren de una alta varianza. Se podría mejorar este hecho recopilando una mayor cantidad de textos para entrenar el modelo y aumentar la probabilidad de dropout de la red para desactivar neuronas y así incrementar el poder de generalización de la red. Aunque sospecho, debido a lo bien que se comporta el modelo sobre el dataframe solo de tweets de enero y diciembre, que estos métodos pueden no funcionar y que sea cuestión de incrementar el preprocesamiento, incluso llegando a incluir las demás features no usadas.\n","\n","Por último, comentar que el buen rendimiento del modelo sobre el dataset preprocesado y solo con los meses de enero y diciembre se podría deber al gran desbalanceo de clases entre ambas etiquetas. Aunque como dato positivo, no existe tanto sobreajuste del modelo a este conjunto de entrenamiento como sobre los anteriores. Se podría usar este modelo en estos meses y sobre los demás predecir que siempre será la otra etiqueta.\n","\n","En caso de haber contado con más tiempo, estas serían las principales vías de investigación que habría seguido para mejorar el modelo, así como retomar el fine-tuning de la arquitectura DeBERTa."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5103994,"sourceId":8543100,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08a878837e944395a2a7646d3b0c202a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ce99c9d15c147a59f52af529c9e3ee1","max":4000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_726f32e6d316417588c6f9b12261740e","value":4000}},"0b610b61452a4b488c34e1967644ee77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18782d1db30241018cdc3c5f95d3376c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8969af8126db46988f7e6c0c9b3fe042","placeholder":"​","style":"IPY_MODEL_ef16b2db4e144f228c40f8ab1ffed538","value":" 4000/4000 [00:01&lt;00:00, 2985.45 examples/s]"}},"19a03011b63643b8bbc316c63312c989":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ce99c9d15c147a59f52af529c9e3ee1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41dc55764e81445a8373b916183a6568":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c35c883ec9b146d5a75c6811a418e38f","placeholder":"​","style":"IPY_MODEL_505dc48c4ec34c3590eb268d6c1e446b","value":" 14400/14400 [00:05&lt;00:00, 2622.78 examples/s]"}},"505dc48c4ec34c3590eb268d6c1e446b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51fc085c513146d189c062d1e7441650":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58e67f2dcbf74ae48b2b87cc81f2f835":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de41b7482e4f487eb6d52c3e579ac5b2","placeholder":"​","style":"IPY_MODEL_bc114b0fcbc648b288d5f37bc513c2e1","value":" 1600/1600 [00:00&lt;00:00, 2562.87 examples/s]"}},"5f1f8dd98229416a9424740b7884a45f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8006459aa544e8fb9f4ecdde128504d","IPY_MODEL_08a878837e944395a2a7646d3b0c202a","IPY_MODEL_18782d1db30241018cdc3c5f95d3376c"],"layout":"IPY_MODEL_0b610b61452a4b488c34e1967644ee77"}},"68a51a31ce314f1b9d43660ec628939a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7159f53434b44c12897e3ab8d290dfe4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"726f32e6d316417588c6f9b12261740e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8969af8126db46988f7e6c0c9b3fe042":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a129bc1d501454084896d9fcc94d603":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8cbdd9d13aec4524bc5469041b7b5f5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d8eb7383c23499a84b3317d5807f9e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98d2312bfd784dd9a4f3d74dc91cae48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7159f53434b44c12897e3ab8d290dfe4","max":1600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc08ea7194ee4045964257eb4be717dc","value":1600}},"ad0ee3bfb6784ae3abd2d70a9858dfec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7cd6c96afea425fac169e251812be9b","placeholder":"​","style":"IPY_MODEL_8d8eb7383c23499a84b3317d5807f9e1","value":"Map: 100%"}},"bc08ea7194ee4045964257eb4be717dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc114b0fcbc648b288d5f37bc513c2e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be58274d14de40d7b473adf77ee62285":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fda818401b2d44b8b13fda3b82a94030","IPY_MODEL_98d2312bfd784dd9a4f3d74dc91cae48","IPY_MODEL_58e67f2dcbf74ae48b2b87cc81f2f835"],"layout":"IPY_MODEL_fd8cf3b87306490aa625a630ece052b2"}},"c35c883ec9b146d5a75c6811a418e38f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8006459aa544e8fb9f4ecdde128504d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19a03011b63643b8bbc316c63312c989","placeholder":"​","style":"IPY_MODEL_8a129bc1d501454084896d9fcc94d603","value":"Map: 100%"}},"c8aa3d624d80400a93597b75c64a7d86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad0ee3bfb6784ae3abd2d70a9858dfec","IPY_MODEL_f3432dd4b46148268a499456fef79a8f","IPY_MODEL_41dc55764e81445a8373b916183a6568"],"layout":"IPY_MODEL_fd1096530fdc4dc3ac1c8f98cb3c8f59"}},"de41b7482e4f487eb6d52c3e579ac5b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e15c758e3e04407a94d91e9f78dd4e22":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7cd6c96afea425fac169e251812be9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef16b2db4e144f228c40f8ab1ffed538":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3432dd4b46148268a499456fef79a8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cbdd9d13aec4524bc5469041b7b5f5d","max":14400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68a51a31ce314f1b9d43660ec628939a","value":14400}},"fd1096530fdc4dc3ac1c8f98cb3c8f59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd8cf3b87306490aa625a630ece052b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fda818401b2d44b8b13fda3b82a94030":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e15c758e3e04407a94d91e9f78dd4e22","placeholder":"​","style":"IPY_MODEL_51fc085c513146d189c062d1e7441650","value":"Map: 100%"}}}}},"nbformat":4,"nbformat_minor":4}
