{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jorge\\Desktop\\MASTER_IA\\TFM\\proyectoCHROMADB\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Device name: NVIDIA RTX A500 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(torch.cuda.get_arch_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'compute_37']\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"type\": {\n",
    "        \"f1\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\"]},\n",
    "        \"recall\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\"]},\n",
    "        \"MulticlassAccuracy\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"None\"]},\n",
    "        \"precision\":{\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\"]}\n",
    "        \n",
    "    },\n",
    "    \"optimize\": {\"name\": \"f1 micro\"}\n",
    "}\n",
    "\n",
    "metrics = {metric: details[\"method\"] for metric, details in metrics[\"type\"].items() if details[\"compute\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': ['micro', 'macro', 'weighted'],\n",
       " 'recall': ['micro', 'macro', 'weighted'],\n",
       " 'MulticlassAccuracy': ['micro', 'macro', 'None'],\n",
       " 'precision': ['micro', 'macro', 'weighted']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_micro\n",
      "f1_macro\n",
      "f1_weighted\n",
      "recall_micro\n",
      "recall_macro\n",
      "recall_weighted\n",
      "MulticlassAccuracy_micro\n",
      "MulticlassAccuracy_macro\n",
      "MulticlassAccuracy_None\n",
      "precision_micro\n",
      "precision_macro\n",
      "precision_weighted\n"
     ]
    }
   ],
   "source": [
    "from transformers import EvalPrediction\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "master_metric_mapper = {\n",
    "    \"f1_micro\": f1_score, # SKLEARN METRIC\n",
    "    \"f1_macro\": f1_score,\n",
    "    \"f1_weighted\": f1_score,\n",
    "    \"f1_binary\": f1_score,\n",
    "    \"MulticlassAccuracy_micro\" : MulticlassAccuracy(average = 'micro', num_classes =  4, k = 1) ,# TORCH METRIC\n",
    "    \"MulticlassAccuracy_macro\" : MulticlassAccuracy(average = 'macro', num_classes = 4, k = 1) ,# TORCH METRIC\n",
    "    \"MulticlassAccuracy_None\" : MulticlassAccuracy(average = None, num_classes =  4, k = 1) ,# TORCH METRIC\n",
    "    \"recall_micro\":recall_score,# SKLEARN METRIC\n",
    "    \"recall_macro\":recall_score,\n",
    "    \"recall_weighted\":recall_score,\n",
    "    \"recall_binary\":recall_score,\n",
    "    \"precision_micro\":precision_score,# SKLEARN METRIC\n",
    "    \"precision_macro\":precision_score,\n",
    "    \"precision_weighted\":precision_score,\n",
    "    \"precision_binary\":precision_score\n",
    "}\n",
    "\n",
    "_metric_obj = {}\n",
    "for metric,methods in metrics.items():\n",
    "    for method in methods:\n",
    "        key_name = metric + \"_\" + method\n",
    "        print(key_name)\n",
    "        metric_obj = master_metric_mapper.get(key_name, None)\n",
    "        if metric_obj is not None:\n",
    "            _metric_obj[key_name] = metric_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_micro': <function sklearn.metrics._classification.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'f1_macro': <function sklearn.metrics._classification.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'f1_weighted': <function sklearn.metrics._classification.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'recall_micro': <function sklearn.metrics._classification.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'recall_macro': <function sklearn.metrics._classification.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'recall_weighted': <function sklearn.metrics._classification.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'MulticlassAccuracy_micro': <torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x14ab8e6ea10>,\n",
       " 'MulticlassAccuracy_macro': <torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x14ab757f210>,\n",
       " 'MulticlassAccuracy_None': <torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x14ab6d9a310>,\n",
       " 'precision_micro': <function sklearn.metrics._classification.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'precision_macro': <function sklearn.metrics._classification.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'precision_weighted': <function sklearn.metrics._classification.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_metric_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 2, 0])\n",
      "tensor([3, 0, 0, 0])\n",
      "0.5\n",
      "0.2\n",
      "0.4\n",
      "0.5\n",
      "0.25\n",
      "0.5\n",
      "0.5\n",
      "0.16666666666666666\n",
      "0.3333333333333333\n",
      "tensor(0.5000)\n",
      "tensor(0.3333)\n",
      "tensor([1., 0., 0., nan])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jorge\\Desktop\\MASTER_IA\\TFM\\proyectoCHROMADB\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Jorge\\Desktop\\MASTER_IA\\TFM\\proyectoCHROMADB\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Jorge\\Desktop\\MASTER_IA\\TFM\\proyectoCHROMADB\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Jorge\\Desktop\\MASTER_IA\\TFM\\proyectoCHROMADB\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_true =torch.tensor([[0.2,0.9,0.9,0.8],\n",
    "                      [0.9,0.2,0.2,0.8],\n",
    "                      [0.2,0.2,0.9,0.8],\n",
    "                      [0.9,0.2,0.2,0.8]\n",
    "                      ])\n",
    "y_true_label =torch.argmax(y_true, dim=1)\n",
    "print(y_true_label)\n",
    "y_pred =torch.tensor([[0.1,0.1,0.1,0.6],\n",
    "                      [0.9,0.3,0.3,0.3],\n",
    "                      [0.3,0.3,0.1,0.1],\n",
    "                      [0.9,0.1,0.3,0.1]\n",
    "                      ])\n",
    "y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "print(y_pred_class)\n",
    "print(_metric_obj[\"f1_micro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"micro\"))\n",
    "print(_metric_obj[\"f1_macro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"macro\"))\n",
    "print(_metric_obj[\"f1_weighted\"](y_true = y_true_label,y_pred = y_pred_class , average=\"weighted\"))\n",
    "print(_metric_obj[\"recall_micro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"micro\"))\n",
    "print(_metric_obj[\"recall_macro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"macro\"))\n",
    "print(_metric_obj[\"recall_weighted\"](y_true = y_true_label,y_pred = y_pred_class , average=\"weighted\"))\n",
    "print(_metric_obj[\"precision_micro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"micro\"))\n",
    "print(_metric_obj[\"precision_macro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"macro\"))\n",
    "print(_metric_obj[\"precision_weighted\"](y_true = y_true_label,y_pred = y_pred_class , average=\"weighted\"))\n",
    "\n",
    "_metric_obj[\"MulticlassAccuracy_micro\"].update(input =y_pred_class, target = y_true_label)\n",
    "_metric_obj[\"MulticlassAccuracy_macro\"].update(input =y_pred, target = y_true_label)\n",
    "_metric_obj[\"MulticlassAccuracy_None\"].update(input =y_pred, target = y_true_label)\n",
    "print(_metric_obj[\"MulticlassAccuracy_micro\"].compute())\n",
    "print(_metric_obj[\"MulticlassAccuracy_macro\"].compute())\n",
    "print(_metric_obj[\"MulticlassAccuracy_None\"].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoeNetVanilla(nn.Module):\n",
    "    def __init__(self, d_model :int , boe_labels : int) -> None:\n",
    "       super().__init__()\n",
    "       self.d_model = d_model # (384)\n",
    "       self.boe_labels = boe_labels # (?)\n",
    "       \n",
    "       # Arquitecture \n",
    "       # (batch, 384)\n",
    "       self.l_1 = nn.Linear(in_features= self.d_model, out_features = 700, bias = True)\n",
    "       self.tan_h = nn.Tanh()\n",
    "       self.drop_1 = nn.Dropout(p = 0.2)\n",
    "       self.l_2 = nn.Linear(in_features= 700, out_features = 1200, bias = True)\n",
    "       self.Relu = nn.ReLU()\n",
    "       self.drop_2 = nn.Dropout(p = 0.3)\n",
    "       # (batch, 10)\n",
    "       self.l_3 = nn.Linear(in_features=  1200 , out_features = self.boe_labels, bias = True)\n",
    "       \n",
    "    def forward(self,x):\n",
    "        h = self.drop_1(self.tan_h(self.l_1(x)))\n",
    "        h = self.drop_2(self.Relu(self.l_2(h)))\n",
    "        return self.l_3(h)\n",
    "    \n",
    "    @staticmethod\n",
    "    def LossFactory():\n",
    "        return nn.CrossEntropyLoss()\n",
    "    \n",
    "    @staticmethod\n",
    "    def OptimizerFactory(model, lr : float = 0.001 , betas : tuple = (0.9, 0.999), eps: float =1e-08):\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr, betas=betas, eps="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x14ab6d9a310>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_metric_obj[\"MulticlassAccuracy_micro\"].reset()\n",
    "_metric_obj[\"MulticlassAccuracy_macro\"].reset()\n",
    "_metric_obj[\"MulticlassAccuracy_None\"].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = t.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0,3].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
