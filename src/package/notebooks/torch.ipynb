{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset.__init__() got an unexpected keyword argument 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Dataset.__init__() got an unexpected keyword argument 'data'"
     ]
    }
   ],
   "source": [
    "ds = Dataset(data = {\"labels\":[1,2,3,4,5], \"features\":[1,2,3,4,5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"type\": {\n",
    "        \"f1\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\"]},\n",
    "        \"recall\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\"]},\n",
    "        \"MulticlassAccuracy\": {\"compute\": True, \"method\": [\"micro\", \"macro\", \"None\"]},\n",
    "        \"precision\":{\"compute\": True, \"method\": [\"micro\", \"macro\", \"weighted\"]}\n",
    "        \n",
    "    },\n",
    "    \"optimize\": {\"name\": \"f1 micro\"}\n",
    "}\n",
    "\n",
    "metrics = {metric: details[\"method\"] for metric, details in metrics[\"type\"].items() if details[\"compute\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': ['micro', 'macro', 'weighted'],\n",
       " 'recall': ['micro', 'macro', 'weighted'],\n",
       " 'MulticlassAccuracy': ['micro', 'macro', 'None'],\n",
       " 'precision': ['micro', 'macro', 'weighted']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_micro\n",
      "f1_macro\n",
      "f1_weighted\n",
      "recall_micro\n",
      "recall_macro\n",
      "recall_weighted\n",
      "MulticlassAccuracy_micro\n",
      "MulticlassAccuracy_macro\n",
      "MulticlassAccuracy_None\n",
      "precision_micro\n",
      "precision_macro\n",
      "precision_weighted\n"
     ]
    }
   ],
   "source": [
    "from transformers import EvalPrediction\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "master_metric_mapper = {\n",
    "    \"f1_micro\": f1_score, # SKLEARN METRIC\n",
    "    \"f1_macro\": f1_score,\n",
    "    \"f1_weighted\": f1_score,\n",
    "    \"f1_binary\": f1_score,\n",
    "    \"MulticlassAccuracy_micro\" : MulticlassAccuracy(average = 'micro', num_classes =  4, k = 1) ,# TORCH METRIC\n",
    "    \"MulticlassAccuracy_macro\" : MulticlassAccuracy(average = 'macro', num_classes = 4, k = 1) ,# TORCH METRIC\n",
    "    \"MulticlassAccuracy_None\" : MulticlassAccuracy(average = None, num_classes =  4, k = 1) ,# TORCH METRIC\n",
    "    \"recall_micro\":recall_score,# SKLEARN METRIC\n",
    "    \"recall_macro\":recall_score,\n",
    "    \"recall_weighted\":recall_score,\n",
    "    \"recall_binary\":recall_score,\n",
    "    \"precision_micro\":precision_score,# SKLEARN METRIC\n",
    "    \"precision_macro\":precision_score,\n",
    "    \"precision_weighted\":precision_score,\n",
    "    \"precision_binary\":precision_score\n",
    "}\n",
    "\n",
    "_metric_obj = {}\n",
    "for metric,methods in metrics.items():\n",
    "    for method in methods:\n",
    "        key_name = metric + \"_\" + method\n",
    "        print(key_name)\n",
    "        metric_obj = master_metric_mapper.get(key_name, None)\n",
    "        if metric_obj is not None:\n",
    "            _metric_obj[key_name] = metric_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_micro': <function sklearn.metrics._classification.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'f1_macro': <function sklearn.metrics._classification.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'f1_weighted': <function sklearn.metrics._classification.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'recall_micro': <function sklearn.metrics._classification.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'recall_macro': <function sklearn.metrics._classification.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'recall_weighted': <function sklearn.metrics._classification.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'MulticlassAccuracy_micro': <torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x14ab8e6ea10>,\n",
       " 'MulticlassAccuracy_macro': <torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x14ab757f210>,\n",
       " 'MulticlassAccuracy_None': <torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x14ab6d9a310>,\n",
       " 'precision_micro': <function sklearn.metrics._classification.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'precision_macro': <function sklearn.metrics._classification.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'precision_weighted': <function sklearn.metrics._classification.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_metric_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 2, 0])\n",
      "tensor([3, 0, 2, 1])\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.25\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "1.0\n",
      "tensor(0.5000)\n",
      "tensor(0.5000)\n",
      "tensor([0.5000,    nan, 0.5000,    nan])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jorge\\Desktop\\MASTER_IA\\TFM\\proyectoCHROMADB\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Jorge\\Desktop\\MASTER_IA\\TFM\\proyectoCHROMADB\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_true =torch.tensor([[0.2,0.2,0.9,0.8],\n",
    "                      [0.9,0.2,0.2,0.8],\n",
    "                      [0.2,0.2,0.9,0.8],\n",
    "                      [0.9,0.2,0.2,0.8]\n",
    "                      ])\n",
    "y_true_label =torch.argmax(y_true, dim=1)\n",
    "print(y_true_label)\n",
    "y_pred =torch.tensor([[0.1,0.1,0.1,0.6],\n",
    "                      [0.9,0.3,0.3,0.3],\n",
    "                      [0.3,0.3,0.1,0.1],\n",
    "                      [0.9,0.1,0.3,0.1]\n",
    "                      ])\n",
    "y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "print(y_pred_class)\n",
    "print(_metric_obj[\"f1_micro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"micro\"))\n",
    "print(_metric_obj[\"f1_macro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"macro\"))\n",
    "print(_metric_obj[\"f1_weighted\"](y_true = y_true_label,y_pred = y_pred_class , average=\"weighted\"))\n",
    "print(_metric_obj[\"recall_micro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"micro\"))\n",
    "print(_metric_obj[\"recall_macro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"macro\"))\n",
    "print(_metric_obj[\"recall_weighted\"](y_true = y_true_label,y_pred = y_pred_class , average=\"weighted\"))\n",
    "print(_metric_obj[\"precision_micro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"micro\"))\n",
    "print(_metric_obj[\"precision_macro\"](y_true = y_true_label,y_pred = y_pred_class , average=\"macro\"))\n",
    "print(_metric_obj[\"precision_weighted\"](y_true = y_true_label,y_pred = y_pred_class , average=\"weighted\"))\n",
    "\n",
    "_metric_obj[\"MulticlassAccuracy_micro\"].update(input =y_pred_class, target = y_true_label)\n",
    "_metric_obj[\"MulticlassAccuracy_macro\"].update(input =y_pred, target = y_true_label)\n",
    "_metric_obj[\"MulticlassAccuracy_None\"].update(input =y_pred, target = y_true_label)\n",
    "print(_metric_obj[\"MulticlassAccuracy_micro\"].compute())\n",
    "print(_metric_obj[\"MulticlassAccuracy_macro\"].compute())\n",
    "print(_metric_obj[\"MulticlassAccuracy_None\"].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torcheval.metrics.classification.accuracy.MulticlassAccuracy at 0x14ab6d9a310>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_metric_obj[\"MulticlassAccuracy_micro\"].reset()\n",
    "_metric_obj[\"MulticlassAccuracy_macro\"].reset()\n",
    "_metric_obj[\"MulticlassAccuracy_None\"].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = t.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0,3].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
